<!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, minimum-scale=1" />
<meta name="generator" content="pdoc 0.10.0" />
<title>Neural.Network API documentation</title>
<meta name="description" content="" />
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/sanitize.min.css" integrity="sha256-PK9q560IAAa6WVRRh76LtCaI8pjTJ2z11v0miyNNjrs=" crossorigin>
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/typography.min.css" integrity="sha256-7l/o7C8jubJiy74VsKTidCy1yBkRtiUGbVkYBylBqUg=" crossorigin>
<link rel="stylesheet preload" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/styles/github.min.css" crossorigin>
<style>:root{--highlight-color:#fe9}.flex{display:flex !important}body{line-height:1.5em}#content{padding:20px}#sidebar{padding:30px;overflow:hidden}#sidebar > *:last-child{margin-bottom:2cm}.http-server-breadcrumbs{font-size:130%;margin:0 0 15px 0}#footer{font-size:.75em;padding:5px 30px;border-top:1px solid #ddd;text-align:right}#footer p{margin:0 0 0 1em;display:inline-block}#footer p:last-child{margin-right:30px}h1,h2,h3,h4,h5{font-weight:300}h1{font-size:2.5em;line-height:1.1em}h2{font-size:1.75em;margin:1em 0 .50em 0}h3{font-size:1.4em;margin:25px 0 10px 0}h4{margin:0;font-size:105%}h1:target,h2:target,h3:target,h4:target,h5:target,h6:target{background:var(--highlight-color);padding:.2em 0}a{color:#058;text-decoration:none;transition:color .3s ease-in-out}a:hover{color:#e82}.title code{font-weight:bold}h2[id^="header-"]{margin-top:2em}.ident{color:#900}pre code{background:#f8f8f8;font-size:.8em;line-height:1.4em}code{background:#f2f2f1;padding:1px 4px;overflow-wrap:break-word}h1 code{background:transparent}pre{background:#f8f8f8;border:0;border-top:1px solid #ccc;border-bottom:1px solid #ccc;margin:1em 0;padding:1ex}#http-server-module-list{display:flex;flex-flow:column}#http-server-module-list div{display:flex}#http-server-module-list dt{min-width:10%}#http-server-module-list p{margin-top:0}.toc ul,#index{list-style-type:none;margin:0;padding:0}#index code{background:transparent}#index h3{border-bottom:1px solid #ddd}#index ul{padding:0}#index h4{margin-top:.6em;font-weight:bold}@media (min-width:200ex){#index .two-column{column-count:2}}@media (min-width:300ex){#index .two-column{column-count:3}}dl{margin-bottom:2em}dl dl:last-child{margin-bottom:4em}dd{margin:0 0 1em 3em}#header-classes + dl > dd{margin-bottom:3em}dd dd{margin-left:2em}dd p{margin:10px 0}.name{background:#eee;font-weight:bold;font-size:.85em;padding:5px 10px;display:inline-block;min-width:40%}.name:hover{background:#e0e0e0}dt:target .name{background:var(--highlight-color)}.name > span:first-child{white-space:nowrap}.name.class > span:nth-child(2){margin-left:.4em}.inherited{color:#999;border-left:5px solid #eee;padding-left:1em}.inheritance em{font-style:normal;font-weight:bold}.desc h2{font-weight:400;font-size:1.25em}.desc h3{font-size:1em}.desc dt code{background:inherit}.source summary,.git-link-div{color:#666;text-align:right;font-weight:400;font-size:.8em;text-transform:uppercase}.source summary > *{white-space:nowrap;cursor:pointer}.git-link{color:inherit;margin-left:1em}.source pre{max-height:500px;overflow:auto;margin:0}.source pre code{font-size:12px;overflow:visible}.hlist{list-style:none}.hlist li{display:inline}.hlist li:after{content:',\2002'}.hlist li:last-child:after{content:none}.hlist .hlist{display:inline;padding-left:1em}img{max-width:100%}td{padding:0 .5em}.admonition{padding:.1em .5em;margin-bottom:1em}.admonition-title{font-weight:bold}.admonition.note,.admonition.info,.admonition.important{background:#aef}.admonition.todo,.admonition.versionadded,.admonition.tip,.admonition.hint{background:#dfd}.admonition.warning,.admonition.versionchanged,.admonition.deprecated{background:#fd4}.admonition.error,.admonition.danger,.admonition.caution{background:lightpink}</style>
<style media="screen and (min-width: 700px)">@media screen and (min-width:700px){#sidebar{width:30%;height:100vh;overflow:auto;position:sticky;top:0}#content{width:70%;max-width:100ch;padding:3em 4em;border-left:1px solid #ddd}pre code{font-size:1em}.item .name{font-size:1em}main{display:flex;flex-direction:row-reverse;justify-content:flex-end}.toc ul ul,#index ul{padding-left:1.5em}.toc > ul > li{margin-top:.5em}}</style>
<style media="print">@media print{#sidebar h1{page-break-before:always}.source{display:none}}@media print{*{background:transparent !important;color:#000 !important;box-shadow:none !important;text-shadow:none !important}a[href]:after{content:" (" attr(href) ")";font-size:90%}a[href][title]:after{content:none}abbr[title]:after{content:" (" attr(title) ")"}.ir a:after,a[href^="javascript:"]:after,a[href^="#"]:after{content:""}pre,blockquote{border:1px solid #999;page-break-inside:avoid}thead{display:table-header-group}tr,img{page-break-inside:avoid}img{max-width:100% !important}@page{margin:0.5cm}p,h2,h3{orphans:3;widows:3}h1,h2,h3,h4,h5,h6{page-break-after:avoid}}</style>
<script defer src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/highlight.min.js" integrity="sha256-Uv3H6lx7dJmRfRvH8TH6kJD1TSK1aFcwgx+mdg3epi8=" crossorigin></script>
<script>window.addEventListener('DOMContentLoaded', () => hljs.initHighlighting())</script>
</head>
<body>
<main>
<article id="content">
<header>
<h1 class="title">Module <code>Neural.Network</code></h1>
</header>
<section id="section-intro">
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python"> ########### used Modules ############

# numpy for linear algebra
import numpy as np

# matplotlib for plotting the loss functions and/or accuracy
import matplotlib.pyplot as plt

# confusion matrix
from sklearn.metrics import confusion_matrix

# accuracy score
from sklearn.metrics import accuracy_score

# show progress bar
from tqdm import tqdm

# Modules for Layer and Utility functions
from .Rectifier import *
from .Layer import *


##### Classes ######


# #### [Batch Normalization class]
class BatchNormalization:
    
    def __init__(self, momentum=0.9, epsilon=1e-6):
        &#39;&#39;&#39;
        Der Konstruktor der BatchNormalization-Klasse. Initialisiert das Momentum und Epsilon.
        
        Parameter:
        momentum: Momentum f√ºr den gleitenden Durchschnitt
        epsilon: ùúñ, Kleine Gleitkommazahl, die zur Varianz hinzugef√ºgt wird, um eine Division durch Null zu vermeiden
        &#39;&#39;&#39;
        self.epsilon = epsilon
        self.momentum = momentum

    def initialize_parameters(self, d):
        &#39;&#39;&#39;
        Diese Funktion initialisiert die Parameter der Batch-Normalisierungsschicht.
        
        Parameter:
        d: Form der Eingabe zur BN-Schicht
        &#39;&#39;&#39;
        self.gamma = np.ones((d))
        self.beta = np.zeros((d))
        self.running_mean = np.zeros((d))
        self.running_var = np.zeros((d))

    def forward(self, z, mode=&#39;train&#39;):
        &#39;&#39;&#39;
        Diese Funktion f√ºhrt die Vorw√§rtspropagation durch. Sie berechnet den Mittelwert und die Varianz der Eingabe, 
        normalisiert die Eingabe und skaliert und verschiebt sie dann.
        
        Parameter:
        z: Eingabe zur BN-Schicht
        mode: Vorw√§rtspass, der f√ºr das Training oder den Test verwendet wird
        
        Ausgabe:
        q: Die normalisierten, skalierten und verschobenen Eingabedaten. Diese Ausgabe wird zur n√§chsten Schicht im Netzwerk weitergeleitet.
        &#39;&#39;&#39;
        if mode==&#39;train&#39;:
            self.batch, self.d = z.shape
            self.mu = np.mean(z, axis = 0) # ùúá
            self.var = np.var(z, axis=0) # ùúé^2
            self.zmu = z - self.mu # z - ùúá
            self.ivar = 1 / np.sqrt(self.var + self.epsilon) # ùúéùëñùëõùë£
            self.zhat = self.zmu * self.ivar
            q = self.gamma*self.zhat + self.beta # ql
            self.running_mean = self.momentum * self.running_mean + (1 - self.momentum) * self.mu
            self.running_var = self.momentum * self.running_var + (1 - self.momentum) * self.var
        elif mode==&#39;test&#39;:
            q = (z - self.running_mean) / np.sqrt(self.running_var + self.epsilon)
            q = self.gamma*q + self.beta
        else:
            raise ValueError(&#39;Invalid forward batchnorm mode &#34;%s&#34;&#39; % mode)
        return q

    def backpropagation(self, dq):
        &#39;&#39;&#39;
        Diese Funktion f√ºhrt die R√ºckw√§rtspropagation durch. Sie berechnet die Gradienten der Skalierungs- und Verschiebungsparameter 
        und den Gradienten der Eingabe.
        
        Parameter:
        dq: Gradient der Ausgabe
        
        Ausgabe:
        dz: Der Gradient der Eingabe zur BN-Schicht. Dieser Gradient wird zur vorherigen Schicht im Netzwerk zur√ºckgegeben und zur Aktualisierung der Gewichte und des Bias in dieser Schicht verwendet.
        &#39;&#39;&#39;
        self.dgamma = np.sum(dq * self.zhat, axis=0)
        self.dbeta = np.sum(dq, axis=0)
        dzhat = dq * self.gamma
        dvar = np.sum(dzhat * self.zmu * (-.5) * (self.ivar**3), axis=0)
        dmu = np.sum(dzhat * (-self.ivar), axis=0)
        dz = dzhat * self.ivar + dvar * (2/self.batch) * self.zmu + (1/self.m)*dmu
        return dz

    def update(self, learnrate, batch, k):
        &#39;&#39;&#39;
        Diese Funktion aktualisiert die Skalierungs- und Verschiebungsparameter basierend auf den w√§hrend der R√ºckw√§rtspropagation 
        berechneten Gradienten.
        
        Parameter:
        learnrate: Lernrate
        batch: Batch-Gr√∂√üe (Anzahl der Proben im Batch)
        k: Iterationsnummer
        &#39;&#39;&#39;
        self.gamma -= self.dgamma*(learnrate/batch)
        self.beta -= self.dbeta*(learnrate/batch)
        
        


# #### Network
#Build an Neural Network

class Network:
    def __init__(self, layers=None):
        &#39;&#39;&#39;
        Erstellt ein sequentielles CNN-Modell.

        Parameters
        ----------
        layers : list, optional
            Eine Liste von Schichten, die dem Modell hinzugef√ºgt werden sollen.
            Wenn None, wird eine leere Liste erstellt. Der Standardwert ist None.
        &#39;&#39;&#39;
        if layers is None:
            self.layers = []
        else:
            self.layers = layers
        self.network_architecture_called = False # Ein Attribut, das angibt, ob die Architektur des Modells berechnet wurde

    def add(self, layer):
        &#39;&#39;&#39;
        F√ºgt eine Schicht zum Modell hinzu.

        Parameters
        ----------
        layer : object
            Ein Objekt, das eine Schicht repr√§sentiert, z.B. Conv2D, Dense, etc.
        &#39;&#39;&#39;
        # F√ºgt die Schicht zur Liste der Schichten hinzu
        self.layers.append(layer)

    def Input(self, input_shape):
        &#39;&#39;&#39;
        Definiert die Eingabeform des Modells.

        Parameters
        ----------
        input_shape : tuple
            Ein Tupel, das die Form der Eingabedaten angibt, z.B. (3, 32, 32) f√ºr RGB-Bilder mit 32x32 Pixeln.
        &#39;&#39;&#39;
        self.d = input_shape # Die Dimension der Eingabe
        self.architecture = [self.d] # Eine Liste, die die Ausgabeform jeder Schicht speichert
        self.layer_name = [&#34;Input&#34;] # Eine Liste, die die Namen jeder Schicht speichert

    def network_architecture(self):
        &#39;&#39;&#39;
        Berechnet die Architektur des Modells basierend auf den hinzugef√ºgten Schichten.
        &#39;&#39;&#39;
        for layer in self.layers: # Iteriert √ºber jede Schicht in der Liste
            if isinstance(layer, Conv2D): # Wenn die Schicht eine Conv2D-Schicht ist
                if layer.input_shape_x is not None: # Wenn die Schicht eine Eingabeform definiert hat
                    self.Input(layer.input_shape_x) # Ruft die Input-Methode mit dieser Form auf
                layer.get_dimensions(self.architecture[-1]) # Berechnet die Ausgabeform der Schicht basierend auf der vorherigen Schicht
                self.architecture.append(layer.output_shape) # F√ºgt die Ausgabeform zur Architekturliste hinzu
                self.layer_name.append(layer.__class__.__name__) # F√ºgt den Namen der Schicht zur Namensliste hinzu
            elif isinstance(layer, (Flatten, Pooling2D)): # Wenn die Schicht eine Flatten- oder Pooling2D-Schicht ist
                layer.get_dimensions(self.architecture[-1]) # Berechnet die Ausgabeform der Schicht basierend auf der vorherigen Schicht
                self.architecture.append(layer.output_shape) # F√ºgt die Ausgabeform zur Architekturliste hinzu
                self.layer_name.append(layer.__class__.__name__) # F√ºgt den Namen der Schicht zur Namensliste hinzu
            elif isinstance(layer, Dense): # Wenn die Schicht eine Dense-Schicht ist
                self.architecture.append(layer.neurons) # F√ºgt die Anzahl der Neuronen zur Architekturliste hinzu
                self.layer_name.append(layer.__class__.__name__) # F√ºgt den Namen der Schicht zur Namensliste hinzu
            else: # Wenn die Schicht eine andere Art von Schicht ist
                self.architecture.append(self.architecture[-1]) # F√ºgt die gleiche Ausgabeform wie die vorherige Schicht zur Architekturliste hinzu
                self.layer_name.append(layer.__class__.__name__) # F√ºgt den Namen der Schicht zur Namensliste hinzu

        self.layers = list(filter(None, self.layers)) # Entfernt alle None-Elemente aus der Schichtenliste
        try:
            idx = self.layer_name.index(&#34;NoneType&#34;) # Sucht nach dem Index eines NoneType-Elements in der Namensliste
            del self.layer_name[idx] # L√∂scht das Element an diesem Index aus der Namensliste
            del self.architecture[idx] # L√∂scht das Element an diesem Index aus der Architekturliste
        except:
            pass # Wenn kein NoneType-Element gefunden wurde, tue nichts

    def summary(self):
        &#39;&#39;&#39;
        Zeigt eine Zusammenfassung des Modells an, einschlie√ülich der Schichttypen, der Ausgabeformen und der Anzahl der Parameter.

        Parameters
        ----------
        self : object
            Eine Instanz der Klasse Network.

        Returns
        -------
        None
            Die Methode gibt nichts zur√ºck, sondern druckt die Zusammenfassung auf dem Bildschirm aus.
        &#39;&#39;&#39;
        if self.network_architecture_called==False: # Wenn die Architektur des Modells noch nicht berechnet wurde
            self.network_architecture() # Ruft die Methode network_architecture auf, um die Architektur zu berechnen
            self.network_architecture_called = True # Setzt das Attribut network_architecture_called auf True
        len_assigned = [45, 26, 15] # Eine Liste von L√§ngen, die f√ºr die Spalten der Zusammenfassung zugewiesen werden
        count = {&#39;Dense&#39;: 1, &#39;Activation&#39;: 1, &#39;Input&#39;: 1,
                &#39;BatchNormalization&#39;: 1, &#39;Dropout&#39;: 1, &#39;Conv2D&#39;: 1,
                &#39;Pooling2D&#39;: 1, &#39;Flatten&#39;: 1} # Ein W√∂rterbuch, das die Anzahl jeder Schichtart speichert

        col_names = [&#39;Layer (type)&#39;, &#39;Output Shape&#39;, &#39;# of Parameters&#39;] # Eine Liste von Spaltennamen f√ºr die Zusammenfassung

        print(&#34;Model: CNN&#34;) # Druckt den Namen des Modells
        print(&#39;-&#39;*sum(len_assigned)) # Druckt eine Trennlinie
        
        text = &#39;&#39; # Initialisiert einen leeren Text
        for i in range(3): # Iteriert √ºber die drei Spalten
            text += col_names[i] + &#39; &#39;*(len_assigned[i]-len(col_names[i])) # F√ºgt den Spaltennamen und die erforderlichen Leerzeichen zum Text hinzu
        print(text) # Druckt den Text

        print(&#39;=&#39;*sum(len_assigned)) # Druckt eine Trennlinie

        total_params = 0 # Initialisiert die Gesamtzahl der Parameter auf 0
        trainable_params = 0 # Initialisiert die Anzahl der trainierbaren Parameter auf 0
        non_trainable_params = 0 # Initialisiert die Anzahl der nicht trainierbaren Parameter auf 0

        for i in range(len(self.layer_name)): # Iteriert √ºber jede Schicht in der Namensliste
            # layer name
            layer_name = self.layer_name[i] # Speichert den Namen der Schicht
            name = layer_name.lower() + &#39;_&#39; + str(count[layer_name]) + &#39; &#39; + &#39;(&#39; + layer_name + &#39;)&#39; # Erstellt einen eindeutigen Namen f√ºr die Schicht mit ihrer Nummer und ihrem Typ
            count[layer_name] += 1 # Erh√∂ht die Anzahl dieser Schichtart um 1

            # output shape
            try: # Versucht, die Ausgabeform der Schicht als Tupel zu erstellen
                out = &#39;(None, &#39; # Beginnt das Tupel mit None f√ºr die Batch-Dimension
                for n in range(len(self.architecture[i])-1): # Iteriert √ºber die restlichen Dimensionen au√üer der letzten
                    out += str(self.architecture[i][n]) + &#39;, &#39; # F√ºgt die Dimension und ein Komma zum Tupel hinzu
                out += str(self.architecture[i][-1]) + &#39;)&#39; # F√ºgt die letzte Dimension und eine schlie√üende Klammer zum Tupel hinzu
            except: # Wenn die Ausgabeform keine Tupel ist
                out = &#39;(None, &#39; + str(self.architecture[i]) + &#39;)&#39; # Erstellt die Ausgabeform als Tupel mit nur einer Dimension

            # number of params
            if layer_name==&#39;Dense&#39;: # Wenn die Schicht eine Dense-Schicht ist
                h0 = self.architecture[i-1] # Speichert die Anzahl der Eingangsneuronen
                h1 = self.architecture[i] # Speichert die Anzahl der Ausgangsneuronen
                if self.layers[i-1].use_bias: # Wenn die Schicht einen Bias-Vektor verwendet
                    params = h0*h1 + h1 # Berechnet die Anzahl der Parameter als das Produkt der Neuronen plus die Anzahl der Ausgangsneuronen
                else: # Wenn die Schicht keinen Bias-Vektor verwendet
                    params = h0*h1 # Berechnet die Anzahl der Parameter als das Produkt der Neuronen
                total_params += params # Addiert die Anzahl der Parameter zur Gesamtzahl hinzu
                trainable_params += params # Addiert die Anzahl der Parameter zur Anzahl der trainierbaren Parameter hinzu
            elif layer_name==&#39;BatchNormalization&#39;: # Wenn die Schicht eine BatchNormalization-Schicht ist
                h = self.architecture[i] # Speichert die Anzahl der Merkmale
                params = 4*h # Berechnet die Anzahl der Parameter als das Vierfache der Merkmale
                trainable_params += 2*h # Addiert die H√§lfte der Parameter zur Anzahl der trainierbaren Parameter hinzu
                non_trainable_params += 2*h # Addiert die H√§lfte der Parameter zur Anzahl der nicht trainierbaren Parameter hinzu
                total_params += params # Addiert die Anzahl der Parameter zur Gesamtzahl hinzu
            elif layer_name==&#39;Conv2D&#39;: # Wenn die Schicht eine Conv2D-Schicht ist
                layer = self.layers[i-1] # Speichert die Schicht als ein Objekt
                if layer.use_bias: # Wenn die Schicht einen Bias-Vektor verwendet
                    add_b = 1 # Speichert eine zus√§tzliche Einheit f√ºr den Bias
                else: # Wenn die Schicht keinen Bias-Vektor verwendet
                    add_b = 0 # Speichert keine zus√§tzliche Einheit f√ºr den Bias
                params = ((layer.inputC * layer.kernelH * layer.kernelW) + add_b) * layer.F # Berechnet die Anzahl der Parameter als das Produkt der Eingangskan√§le, der Kernelh√∂he, der Kernelbreite und der Anzahl der Filter plus die zus√§tzliche Einheit f√ºr den Bias
                trainable_params += params # Addiert die Anzahl der Parameter zur Anzahl der trainierbaren Parameter hinzu
                total_params += params # Addiert die Anzahl der Parameter zur Gesamtzahl hinzu
            else: # Wenn die Schicht eine andere Art von Schicht ist
                # Pooling, Dropout, Flatten, Input
                params = 0 # Speichert die Anzahl der Parameter als 0
            names = [name, out, str(params)] # Erstellt eine Liste mit dem Namen, der Ausgabeform und der Anzahl der Parameter der Schicht

            # print this row
            text = &#39;&#39; # Initialisiert einen leeren Text
            for j in range(3): # Iteriert √ºber die drei Spalten
                text += names[j] + &#39; &#39;*(len_assigned[j]-len(names[j])) # F√ºgt den Namen, die Ausgabeform oder die Anzahl der Parameter und die erforderlichen Leerzeichen zum Text hinzu
            print(text) # Druckt den Text
            if i!=(len(self.layer_name)-1): # Wenn dies nicht die letzte Schicht ist
                print(&#39;-&#39;*sum(len_assigned)) # Druckt eine Trennlinie
            else: # Wenn dies die letzte Schicht ist
                print(&#39;=&#39;*sum(len_assigned)) # Druckt eine Trennlinie

        print(&#34;Total params:&#34;, total_params) # Druckt die Gesamtzahl der Parameter
        print(&#34;Trainable params:&#34;, trainable_params) # Druckt die Anzahl der trainierbaren Parameter
        print(&#34;Non-trainable params:&#34;, non_trainable_params) # Druckt die Anzahl der nicht trainierbaren Parameter
        print(&#39;-&#39;*sum(len_assigned)) # Druckt eine Trennlinie
    
    def compile(self, cost_type, optimizer_type):
        &#39;&#39;&#39;
        Kompiliert das Modell mit einer Kostenfunktion und einem Optimierer.

        Parameters
        ----------
        self : object
            Eine Instanz der Klasse Network.
        cost_type : str
            Der Name der Kostenfunktion, die f√ºr das Modell verwendet werden soll, z.B. &#34;cross-entropy&#34; oder &#34;mse&#34;.
        optimizer_type : str
            Der Name des Optimierers, der f√ºr das Modell verwendet werden soll, z.B. &#34;sgd&#34; oder &#34;adam&#34;.

        Returns
        -------
        None
            Die Methode gibt nichts zur√ºck, sondern setzt die Attribute cost, cost_type und optimizer_type der Instanz.
        &#39;&#39;&#39;
        self.cost = Cost(cost_type) # Erstellt ein Objekt der Klasse Cost mit der angegebenen Kostenfunktion
        self.cost_type = cost_type # Speichert den Namen der Kostenfunktion als Attribut
        self.optimizer_type = optimizer_type # Speichert den Namen des Optimierers als Attribut

    def initialize_parameters(self):
        &#39;&#39;&#39;
        Initialisiert die Parameter des Modells basierend auf den hinzugef√ºgten Schichten.

        Parameters
        ----------
        self : object
            Eine Instanz der Klasse Network.

        Returns
        -------
        None
            Die Methode gibt nichts zur√ºck, sondern setzt die Parameter der Schichten als Attribute der Instanz.
        &#39;&#39;&#39;
        if not self.network_architecture_called: # Wenn die Architektur des Modells noch nicht berechnet wurde
            self.network_architecture() # Ruft die Methode network_architecture auf, um die Architektur zu berechnen
            self.network_architecture_called = True # Setzt das Attribut network_architecture_called auf True
        for i, layer in enumerate(self.layers): # Iteriert √ºber jede Schicht in der Liste der Schichten
            if isinstance(layer, (Dense, Conv2D)): # Wenn die Schicht eine Dense- oder Conv2D-Schicht ist
                #print(&#34;Layer: &#34;, layer.__class__.__name__, &#34; input: &#34;, self.architecture[i])
                layer.initialize_parameters(self.architecture[i], self.optimizer_type) # Ruft die Methode initialize_parameters der Schicht auf, um die Parameter zu initialisieren
            elif isinstance(layer, BatchNormalization): # Wenn die Schicht eine BatchNormalization-Schicht ist
                layer.initialize_parameters(self.architecture[i]) # Ruft die Methode initialize_parameters der Schicht auf, um die Parameter zu initialisieren


    
    def fit(self, X, y, epochs=10, batch_size=5, learnrate=1, X_val=None, y_val=None, verbose=1, learnrate_decay=None, **kwargs):
        &#39;&#39;&#39;
        Trainiert das Modell mit den gegebenen Trainingsdaten und optionalen Validierungsdaten.

        Parameters
        ----------
        self : object
            Eine Instanz der Klasse Network.
        X : array-like
            Die Eingabedaten f√ºr das Training, z.B. ein Numpy-Array oder eine Liste von Arrays.
        y : array-like
            Die Ausgabedaten f√ºr das Training, z.B. ein Numpy-Array oder eine Liste von Arrays.
        epochs : int, optional
            Die Anzahl der Epochen, die das Modell trainieren soll. Der Standardwert ist 10.
        batch_size : int, optional
            Die Gr√∂√üe der Minibatches, die f√ºr das Training verwendet werden sollen. Der Standardwert ist 5.
        learnrate : float, optional
            Die Lernrate, die f√ºr den Optimierer verwendet werden soll. Der Standardwert ist 1.
        X_val : array-like, optional
            Die Eingabedaten f√ºr die Validierung, z.B. ein Numpy-Array oder eine Liste von Arrays. Wenn None, wird keine Validierung durchgef√ºhrt. Der Standardwert ist None.
        y_val : array-like, optional
            Die Ausgabedaten f√ºr die Validierung, z.B. ein Numpy-Array oder eine Liste von Arrays. Wenn None, wird keine Validierung durchgef√ºhrt. Der Standardwert ist None.
        verbose : int, optional
            Ein Schalter, der angibt, ob die Trainings- und Validierungsergebnisse nach jeder Epoche gedruckt werden sollen. Wenn 1, werden die Ergebnisse gedruckt. Wenn 0, werden die Ergebnisse nicht gedruckt. Der Standardwert ist 1.
        learnrate_decay : function, optional
            Eine Funktion, die die Lernrate nach jeder Iteration anpasst. Wenn None, wird keine Lernratenanpassung durchgef√ºhrt. Der Standardwert ist None.
        **kwargs : dict, optional
            Zus√§tzliche Schl√ºsselwortargumente, die an die Funktion learnrate_decay √ºbergeben werden sollen.

        Returns
        -------
        None
            Die Methode gibt nichts zur√ºck, sondern aktualisiert die Parameter des Modells und speichert die Trainings- und Validierungshistorie als Attribute der Instanz.
        &#39;&#39;&#39;
        self.history = {&#39;Training Loss&#39;: [],&#39;Validation Loss&#39;: [], &#39;Training Accuracy&#39;: [],  &#39;Validation Accuracy&#39;: []} # Erstellt ein W√∂rterbuch, das die Trainings- und Validierungshistorie speichert
        iterations = 0 # Initialisiert die Anzahl der Iterationen auf 0
        self.batch = batch_size # Speichert die Gr√∂√üe der Minibatches als Attribut
        self.initialize_parameters() # Ruft die Methode initialize_parameters auf, um die Parameter des Modells zu initialisieren
        total_num_batches = np.ceil(len(X)/batch_size) # Berechnet die Gesamtzahl der Minibatches

        for epoch in range(epochs): # Iteriert √ºber jede Epoche
            cost_train = 0 # Initialisiert die Trainingskosten auf 0
            num_batches = 0 # Initialisiert die Anzahl der Minibatches auf 0
            y_pred_train = [] # Initialisiert eine Liste, die die Vorhersagen des Modells f√ºr die Trainingsdaten speichert
            y_train = [] # Initialisiert eine Liste, die die tats√§chlichen Ausgaben f√ºr die Trainingsdaten speichert

            print(f&#39;\nEpoch: {epoch+1}/{epochs}&#39;) # Druckt die aktuelle Epoche

            for i in tqdm(range(0, len(X), batch_size)): # Iteriert √ºber jede Minibatch
                X_batch = X[i:i+batch_size] # Extrahiert die Eingabedaten f√ºr die Minibatch
                y_batch = y[i:i+batch_size] # Extrahiert die Ausgabedaten f√ºr die Minibatch

                Z = X_batch.copy() # Erstellt eine Kopie der Eingabedaten

                # feed-forward
                for layer in self.layers: # Iteriert √ºber jede Schicht im Modell
                    Z = layer.forward(Z) # Ruft die Methode forward der Schicht auf, um die Ausgabe der Schicht zu berechnen

                # calculating training accuracy
                if self.cost_type==&#39;cross-entropy&#39;: # Wenn die Kostenfunktion die Kreuzentropie ist
                    y_pred_train += np.argmax(Z, axis=1).tolist() # F√ºgt die Vorhersagen des Modells f√ºr die Minibatch zur Liste der Vorhersagen hinzu
                    y_train += np.argmax(y_batch, axis=1).tolist() # F√ºgt die tats√§chlichen Ausgaben f√ºr die Minibatch zur Liste der Ausgaben hinzu

                # calculating the loss
                cost_train += self.cost.get_cost(Z, y_batch) / self.batch # Berechnet die Kosten f√ºr die Minibatch und addiert sie zu den Trainingskosten

                # calculating dL/daL (last layer backprop error)
                dZ = self.cost.get_d_cost(Z, y_batch) # Berechnet den Fehler der letzten Schicht
                # backpropagation
                for layer in self.layers[::-1]: # Iteriert √ºber jede Schicht im Modell in umgekehrter Reihenfolge
                    dZ = layer.backpropagation(dZ) # Ruft die Methode backpropagation der Schicht auf, um den Fehler an die vorherige Schicht weiterzugeben

                # Parameters update
                for layer in self.layers: # Iteriert √ºber jede Schicht im Modell
                    if isinstance(layer, (Dense, BatchNormalization, Conv2D)): # Wenn die Schicht eine Dense-, BatchNormalization- oder Conv2D-Schicht ist
                        layer.update(learnrate, self.batch, iterations) # Ruft die Methode update der Schicht auf, um die Parameter der Schicht zu aktualisieren

                # Learning rate decay
                if learnrate_decay is not None: # Wenn eine Lernratenanpassungsfunktion angegeben ist
                    learnrate = learnrate_decay(iterations, **kwargs) # Ruft die Funktion learnrate_decay auf, um die Lernrate anzupassen

                num_batches += 1 # Erh√∂ht die Anzahl der Minibatches um 1
                iterations += 1 # Erh√∂ht die Anzahl der Iterationen um 1

            cost_train /= num_batches # Berechnet den Durchschnitt der Trainingskosten f√ºr die Epoche

            # printing purpose only (Training Accuracy, Validation loss and accuracy)

            text  = f&#39;Training Loss: {round(cost_train, 4)} - &#39; # Erstellt einen Text, der die Trainingskosten enth√§lt
            self.history[&#39;Training Loss&#39;].append(cost_train) # F√ºgt die Trainingskosten zur Historie hinzu

            # training accuracy

            if self.cost_type==&#39;cross-entropy&#39;: # Wenn die Kostenfunktion die Kreuzentropie ist
                accuracy_train = np.sum(np.array(y_pred_train) == np.array(y_train)) / len(y_train) # Berechnet die Trainingsgenauigkeit f√ºr die Epoche
                text += f&#39;Training Accuracy: {round(accuracy_train, 4)}&#39; # F√ºgt die Trainingsgenauigkeit zum Text hinzu
                self.history[&#39;Training Accuracy&#39;].append(accuracy_train) # F√ºgt die Trainingsgenauigkeit zur Historie hinzu
            else: # Wenn die Kostenfunktion eine andere ist
                text += f&#39;Training Accuracy: {round(cost_train, 4)}&#39; # F√ºgt die Trainingskosten als Genauigkeit zum Text hinzu
                self.history[&#39;Training Accuracy&#39;].append(cost_train) # F√ºgt die Trainingskosten als Genauigkeit zur Historie hinzu

            if X_val is not None: # Wenn Validierungsdaten angegeben sind
                cost_val, accuracy_val = self.evaluate(X_val, y_val, batch_size) # Ruft die Methode evaluate auf, um die Validierungskosten und -genauigkeit zu berechnen
                text += f&#39; - Validation Loss: {round(cost_val, 4)} - &#39; # F√ºgt die Validierungskosten zum Text hinzu
                self.history[&#39;Validation Loss&#39;].append(cost_val) # F√ºgt die Validierungskosten zur Historie hinzu
                text += f&#39;Validation Accuracy: {round(accuracy_val, 4)}&#39; # F√ºgt die Validierungsgenauigkeit zum Text hinzu
                self.history[&#39;Validation Accuracy&#39;].append(accuracy_val) # F√ºgt die Validierungsgenauigkeit zur Historie hinzu

            if verbose:
                    print(text)
            else:
                print()
    
    def evaluate(self, X, y, batch_size=None):
        &#39;&#39;&#39;
        Bewertet das Modell mit den gegebenen Testdaten.

        Parameters
        ----------
        self : object
            Eine Instanz der Klasse Network.
        X : array-like
            Die Eingabedaten f√ºr den Test, z.B. ein Numpy-Array oder eine Liste von Arrays.
        y : array-like
            Die Ausgabedaten f√ºr den Test, z.B. ein Numpy-Array oder eine Liste von Arrays.
        batch_size : int, optional
            Die Gr√∂√üe der Minibatches, die f√ºr den Test verwendet werden sollen. Wenn None, wird die L√§nge von X verwendet. Der Standardwert ist None.

        Returns
        -------
        cost : float
            Die Kosten des Modells f√ºr die Testdaten, berechnet mit der Kostenfunktion des Modells.
        accuracy : float
            Die Genauigkeit des Modells f√ºr die Testdaten, berechnet als der Anteil der korrekten Vorhersagen.
        &#39;&#39;&#39;
        if batch_size is None: # Wenn keine Batch-Gr√∂√üe angegeben ist
            batch_size = len(X) # Verwendet die L√§nge von X als Batch-Gr√∂√üe

        cost = 0 # Initialisiert die Kosten auf 0
        correct = 0 # Initialisiert die Anzahl der korrekten Vorhersagen auf 0
        num_batches = 0 # Initialisiert die Anzahl der Minibatches auf 0
        utility = Utility() # Erstellt ein Objekt der Klasse Utility
        Y_1hot, _ = utility.onehot(y) # Wandelt die Ausgabedaten in One-Hot-Vektoren um

        for i in range(0, len(X), batch_size): # Iteriert √ºber jede Minibatch
            X_batch = X[i:i+batch_size] # Extrahiert die Eingabedaten f√ºr die Minibatch
            y_batch = y[i:i+batch_size] # Extrahiert die Ausgabedaten f√ºr die Minibatch
            Y_1hot_batch = Y_1hot[i:i+batch_size] # Extrahiert die One-Hot-Vektoren f√ºr die Minibatch
            Z = X_batch.copy() # Erstellt eine Kopie der Eingabedaten
            for layer in self.layers: # Iteriert √ºber jede Schicht im Modell
                if layer.__class__.__name__==&#39;BatchNormalization&#39;: # Wenn die Schicht eine BatchNormalization-Schicht ist
                    Z = layer.forward(Z, mode=&#39;test&#39;) # Ruft die Methode forward der Schicht auf, um die Ausgabe der Schicht im Testmodus zu berechnen
                else: # Wenn die Schicht eine andere Art von Schicht ist
                    Z = layer.forward(Z) # Ruft die Methode forward der Schicht auf, um die Ausgabe der Schicht zu berechnen
            if self.cost_type==&#39;cross-entropy&#39;: # Wenn die Kostenfunktion die Kreuzentropie ist
                cost += self.cost.get_cost(Z, Y_1hot_batch) / len(y_batch) # Berechnet die Kosten f√ºr die Minibatch und addiert sie zu den Gesamtkosten
                y_pred = np.argmax(Z, axis=1).tolist() # Berechnet die Vorhersagen des Modells f√ºr die Minibatch
                correct += np.sum(y_pred == y_batch) # Z√§hlt die Anzahl der korrekten Vorhersagen f√ºr die Minibatch
            else: # Wenn die Kostenfunktion eine andere ist
                cost += self.cost.get_cost(Z, y_batch) / len(y_batch) # Berechnet die Kosten f√ºr die Minibatch und addiert sie zu den Gesamtkosten

            num_batches += 1 # Erh√∂ht die Anzahl der Minibatches um 1

        if self.cost_type==&#39;cross-entropy&#39;: # Wenn die Kostenfunktion die Kreuzentropie ist
            accuracy = correct / len(y) # Berechnet die Genauigkeit des Modells f√ºr die Testdaten
            cost /= num_batches # Berechnet den Durchschnitt der Kosten f√ºr die Testdaten
            return cost, accuracy # Gibt die Kosten und die Genauigkeit zur√ºck
        else: # Wenn die Kostenfunktion eine andere ist
            cost /= num_batches # Berechnet den Durchschnitt der Kosten f√ºr die Testdaten
            return cost, cost # Gibt die Kosten zweimal zur√ºck

    def loss_plot(self):
        &#39;&#39;&#39;
        Zeigt einen Plot der Trainings- und Validierungskosten pro Epoche an.

        Parameters
        ----------
        self : object
            Eine Instanz der Klasse Network.

        Returns
        -------
        None
            Die Methode gibt nichts zur√ºck, sondern zeigt den Plot auf dem Bildschirm an.
        &#39;&#39;&#39;
        plt.plot(self.history[&#39;Training Loss&#39;], &#39;k&#39;) # Plottet die Trainingskosten in schwarz
        if len(self.history[&#39;Validation Loss&#39;])&gt;0: # Wenn es Validierungskosten gibt
            plt.plot(self.history[&#39;Validation Loss&#39;], &#39;r&#39;) # Plottet die Validierungskosten in rot
            plt.legend([&#39;Train&#39;, &#39;Validation&#39;], loc=&#39;upper right&#39;) # F√ºgt eine Legende mit den Namen der Kurven hinzu
            plt.title(&#39;Model Loss&#39;) # F√ºgt einen Titel f√ºr den Plot hinzu
        else: # Wenn es keine Validierungskosten gibt
            plt.title(&#39;Training Loss&#39;) # F√ºgt einen Titel f√ºr den Plot hinzu
        plt.ylabel(&#39;Loss&#39;) # F√ºgt eine Beschriftung f√ºr die y-Achse hinzu
        plt.xlabel(&#39;Epoch&#39;) # F√ºgt eine Beschriftung f√ºr die x-Achse hinzu
        plt.show() # Zeigt den Plot an

    def accuracy_plot(self):
        &#39;&#39;&#39;
        Zeigt einen Plot der Trainings- und Validierungsgenauigkeit pro Epoche an.

        Parameters
        ----------
        self : object
            Eine Instanz der Klasse Network.

        Returns
        -------
        None
            Die Methode gibt nichts zur√ºck, sondern zeigt den Plot auf dem Bildschirm an.
        &#39;&#39;&#39;
        plt.plot(self.history[&#39;Training Accuracy&#39;], &#39;k&#39;) # Plottet die Trainingsgenauigkeit in schwarz
        if len(self.history[&#39;Validation Accuracy&#39;])&gt;0: # Wenn es Validierungsgenauigkeit gibt
            plt.plot(self.history[&#39;Validation Accuracy&#39;], &#39;r&#39;) # Plottet die Validierungsgenauigkeit in rot
            plt.legend([&#39;Train&#39;, &#39;Validation&#39;], loc=&#39;lower right&#39;) # F√ºgt eine Legende mit den Namen der Kurven hinzu
            plt.title(&#39;Model Accuracy&#39;) # F√ºgt einen Titel f√ºr den Plot hinzu
        else: # Wenn es keine Validierungsgenauigkeit gibt
            plt.title(&#39;Training Accuracy&#39;) # F√ºgt einen Titel f√ºr den Plot hinzu
        plt.ylabel(&#39;Accuracy&#39;) # F√ºgt eine Beschriftung f√ºr die y-Achse hinzu
        plt.xlabel(&#39;Epoch&#39;) # F√ºgt eine Beschriftung f√ºr die x-Achse hinzu
        plt.show() # Zeigt den Plot an

    def predict(self, X, batch_size=None):
        &#39;&#39;&#39;
        Erzeugt Vorhersagen des Modells f√ºr die gegebenen Eingabedaten.

        Parameters
        ----------
        self : object
            Eine Instanz der Klasse Network.
        X : array-like
            Die Eingabedaten, f√ºr die das Modell Vorhersagen machen soll, z.B. ein Numpy-Array oder eine Liste von Arrays.
        batch_size : int, optional
            Die Gr√∂√üe der Minibatches, die f√ºr die Vorhersage verwendet werden sollen. Wenn None, wird die L√§nge von X verwendet. Der Standardwert ist None.

        Returns
        -------
        y_pred : array-like
            Die Vorhersagen des Modells f√ºr die Eingabedaten, z.B. ein Numpy-Array oder eine Liste von Arrays.
        &#39;&#39;&#39;
        if batch_size==None: # Wenn keine Batch-Gr√∂√üe angegeben ist
            batch_size = len(X) # Verwendet die L√§nge von X als Batch-Gr√∂√üe

        for i in range(0, len(X), batch_size): # Iteriert √ºber jede Minibatch
            X_batch = X[i:i+batch_size] # Extrahiert die Eingabedaten f√ºr die Minibatch
            Z = X_batch.copy() # Erstellt eine Kopie der Eingabedaten
            for layer in self.layers: # Iteriert √ºber jede Schicht im Modell
                if layer.__class__.__name__==&#39;BatchNormalization&#39;: # Wenn die Schicht eine BatchNormalization-Schicht ist
                    Z = layer.forward(Z, mode=&#39;test&#39;) # Ruft die Methode forward der Schicht auf, um die Ausgabe der Schicht im Testmodus zu berechnen
                else: # Wenn die Schicht eine andere Art von Schicht ist
                    Z = layer.forward(Z) # Ruft die Methode forward der Schicht auf, um die Ausgabe der Schicht zu berechnen
            if i==0: # Wenn dies die erste Minibatch ist
                if self.cost_type==&#39;cross-entropy&#39;: # Wenn die Kostenfunktion die Kreuzentropie ist
                    y_pred = np.argmax(Z, axis=1).tolist() # Berechnet die Vorhersagen des Modells f√ºr die Minibatch als eine Liste von Indizes
                else: # Wenn die Kostenfunktion eine andere ist
                    y_pred = Z # Speichert die Ausgabe des Modells f√ºr die Minibatch als ein Array
            else: # Wenn dies nicht die erste Minibatch ist
                if self.cost_type==&#39;cross-entropy&#39;: # Wenn die Kostenfunktion die Kreuzentropie ist
                    y_pred += np.argmax(Z, axis=1).tolist() # F√ºgt die Vorhersagen des Modells f√ºr die Minibatch zur Liste der Vorhersagen hinzu
                else: # Wenn die Kostenfunktion eine andere ist
                    y_pred = np.vstack((y_pred, Z)) # Stapelt die Ausgabe des Modells f√ºr die Minibatch unter der bisherigen Ausgabe

        return np.array(y_pred) # Gibt die Vorhersagen des Modells f√ºr die Eingabedaten als ein Array zur√ºck
    
    </code></pre>
</details>
</section>
<section>
</section>
<section>
</section>
<section>
</section>
<section>
<h2 class="section-title" id="header-classes">Classes</h2>
<dl>
<dt id="Neural.Network.BatchNormalization"><code class="flex name class">
<span>class <span class="ident">BatchNormalization</span></span>
<span>(</span><span>momentum=0.9, epsilon=1e-06)</span>
</code></dt>
<dd>
<div class="desc"><p>Der Konstruktor der BatchNormalization-Klasse. Initialisiert das Momentum und Epsilon.</p>
<p>Parameter:
momentum: Momentum f√ºr den gleitenden Durchschnitt
epsilon: ùúñ, Kleine Gleitkommazahl, die zur Varianz hinzugef√ºgt wird, um eine Division durch Null zu vermeiden</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class BatchNormalization:
    
    def __init__(self, momentum=0.9, epsilon=1e-6):
        &#39;&#39;&#39;
        Der Konstruktor der BatchNormalization-Klasse. Initialisiert das Momentum und Epsilon.
        
        Parameter:
        momentum: Momentum f√ºr den gleitenden Durchschnitt
        epsilon: ùúñ, Kleine Gleitkommazahl, die zur Varianz hinzugef√ºgt wird, um eine Division durch Null zu vermeiden
        &#39;&#39;&#39;
        self.epsilon = epsilon
        self.momentum = momentum

    def initialize_parameters(self, d):
        &#39;&#39;&#39;
        Diese Funktion initialisiert die Parameter der Batch-Normalisierungsschicht.
        
        Parameter:
        d: Form der Eingabe zur BN-Schicht
        &#39;&#39;&#39;
        self.gamma = np.ones((d))
        self.beta = np.zeros((d))
        self.running_mean = np.zeros((d))
        self.running_var = np.zeros((d))

    def forward(self, z, mode=&#39;train&#39;):
        &#39;&#39;&#39;
        Diese Funktion f√ºhrt die Vorw√§rtspropagation durch. Sie berechnet den Mittelwert und die Varianz der Eingabe, 
        normalisiert die Eingabe und skaliert und verschiebt sie dann.
        
        Parameter:
        z: Eingabe zur BN-Schicht
        mode: Vorw√§rtspass, der f√ºr das Training oder den Test verwendet wird
        
        Ausgabe:
        q: Die normalisierten, skalierten und verschobenen Eingabedaten. Diese Ausgabe wird zur n√§chsten Schicht im Netzwerk weitergeleitet.
        &#39;&#39;&#39;
        if mode==&#39;train&#39;:
            self.batch, self.d = z.shape
            self.mu = np.mean(z, axis = 0) # ùúá
            self.var = np.var(z, axis=0) # ùúé^2
            self.zmu = z - self.mu # z - ùúá
            self.ivar = 1 / np.sqrt(self.var + self.epsilon) # ùúéùëñùëõùë£
            self.zhat = self.zmu * self.ivar
            q = self.gamma*self.zhat + self.beta # ql
            self.running_mean = self.momentum * self.running_mean + (1 - self.momentum) * self.mu
            self.running_var = self.momentum * self.running_var + (1 - self.momentum) * self.var
        elif mode==&#39;test&#39;:
            q = (z - self.running_mean) / np.sqrt(self.running_var + self.epsilon)
            q = self.gamma*q + self.beta
        else:
            raise ValueError(&#39;Invalid forward batchnorm mode &#34;%s&#34;&#39; % mode)
        return q

    def backpropagation(self, dq):
        &#39;&#39;&#39;
        Diese Funktion f√ºhrt die R√ºckw√§rtspropagation durch. Sie berechnet die Gradienten der Skalierungs- und Verschiebungsparameter 
        und den Gradienten der Eingabe.
        
        Parameter:
        dq: Gradient der Ausgabe
        
        Ausgabe:
        dz: Der Gradient der Eingabe zur BN-Schicht. Dieser Gradient wird zur vorherigen Schicht im Netzwerk zur√ºckgegeben und zur Aktualisierung der Gewichte und des Bias in dieser Schicht verwendet.
        &#39;&#39;&#39;
        self.dgamma = np.sum(dq * self.zhat, axis=0)
        self.dbeta = np.sum(dq, axis=0)
        dzhat = dq * self.gamma
        dvar = np.sum(dzhat * self.zmu * (-.5) * (self.ivar**3), axis=0)
        dmu = np.sum(dzhat * (-self.ivar), axis=0)
        dz = dzhat * self.ivar + dvar * (2/self.batch) * self.zmu + (1/self.m)*dmu
        return dz

    def update(self, learnrate, batch, k):
        &#39;&#39;&#39;
        Diese Funktion aktualisiert die Skalierungs- und Verschiebungsparameter basierend auf den w√§hrend der R√ºckw√§rtspropagation 
        berechneten Gradienten.
        
        Parameter:
        learnrate: Lernrate
        batch: Batch-Gr√∂√üe (Anzahl der Proben im Batch)
        k: Iterationsnummer
        &#39;&#39;&#39;
        self.gamma -= self.dgamma*(learnrate/batch)
        self.beta -= self.dbeta*(learnrate/batch)</code></pre>
</details>
<h3>Methods</h3>
<dl>
<dt id="Neural.Network.BatchNormalization.backpropagation"><code class="name flex">
<span>def <span class="ident">backpropagation</span></span>(<span>self, dq)</span>
</code></dt>
<dd>
<div class="desc"><p>Diese Funktion f√ºhrt die R√ºckw√§rtspropagation durch. Sie berechnet die Gradienten der Skalierungs- und Verschiebungsparameter
und den Gradienten der Eingabe.</p>
<p>Parameter:
dq: Gradient der Ausgabe</p>
<p>Ausgabe:
dz: Der Gradient der Eingabe zur BN-Schicht. Dieser Gradient wird zur vorherigen Schicht im Netzwerk zur√ºckgegeben und zur Aktualisierung der Gewichte und des Bias in dieser Schicht verwendet.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def backpropagation(self, dq):
    &#39;&#39;&#39;
    Diese Funktion f√ºhrt die R√ºckw√§rtspropagation durch. Sie berechnet die Gradienten der Skalierungs- und Verschiebungsparameter 
    und den Gradienten der Eingabe.
    
    Parameter:
    dq: Gradient der Ausgabe
    
    Ausgabe:
    dz: Der Gradient der Eingabe zur BN-Schicht. Dieser Gradient wird zur vorherigen Schicht im Netzwerk zur√ºckgegeben und zur Aktualisierung der Gewichte und des Bias in dieser Schicht verwendet.
    &#39;&#39;&#39;
    self.dgamma = np.sum(dq * self.zhat, axis=0)
    self.dbeta = np.sum(dq, axis=0)
    dzhat = dq * self.gamma
    dvar = np.sum(dzhat * self.zmu * (-.5) * (self.ivar**3), axis=0)
    dmu = np.sum(dzhat * (-self.ivar), axis=0)
    dz = dzhat * self.ivar + dvar * (2/self.batch) * self.zmu + (1/self.m)*dmu
    return dz</code></pre>
</details>
</dd>
<dt id="Neural.Network.BatchNormalization.forward"><code class="name flex">
<span>def <span class="ident">forward</span></span>(<span>self, z, mode='train')</span>
</code></dt>
<dd>
<div class="desc"><p>Diese Funktion f√ºhrt die Vorw√§rtspropagation durch. Sie berechnet den Mittelwert und die Varianz der Eingabe,
normalisiert die Eingabe und skaliert und verschiebt sie dann.</p>
<p>Parameter:
z: Eingabe zur BN-Schicht
mode: Vorw√§rtspass, der f√ºr das Training oder den Test verwendet wird</p>
<p>Ausgabe:
q: Die normalisierten, skalierten und verschobenen Eingabedaten. Diese Ausgabe wird zur n√§chsten Schicht im Netzwerk weitergeleitet.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def forward(self, z, mode=&#39;train&#39;):
    &#39;&#39;&#39;
    Diese Funktion f√ºhrt die Vorw√§rtspropagation durch. Sie berechnet den Mittelwert und die Varianz der Eingabe, 
    normalisiert die Eingabe und skaliert und verschiebt sie dann.
    
    Parameter:
    z: Eingabe zur BN-Schicht
    mode: Vorw√§rtspass, der f√ºr das Training oder den Test verwendet wird
    
    Ausgabe:
    q: Die normalisierten, skalierten und verschobenen Eingabedaten. Diese Ausgabe wird zur n√§chsten Schicht im Netzwerk weitergeleitet.
    &#39;&#39;&#39;
    if mode==&#39;train&#39;:
        self.batch, self.d = z.shape
        self.mu = np.mean(z, axis = 0) # ùúá
        self.var = np.var(z, axis=0) # ùúé^2
        self.zmu = z - self.mu # z - ùúá
        self.ivar = 1 / np.sqrt(self.var + self.epsilon) # ùúéùëñùëõùë£
        self.zhat = self.zmu * self.ivar
        q = self.gamma*self.zhat + self.beta # ql
        self.running_mean = self.momentum * self.running_mean + (1 - self.momentum) * self.mu
        self.running_var = self.momentum * self.running_var + (1 - self.momentum) * self.var
    elif mode==&#39;test&#39;:
        q = (z - self.running_mean) / np.sqrt(self.running_var + self.epsilon)
        q = self.gamma*q + self.beta
    else:
        raise ValueError(&#39;Invalid forward batchnorm mode &#34;%s&#34;&#39; % mode)
    return q</code></pre>
</details>
</dd>
<dt id="Neural.Network.BatchNormalization.initialize_parameters"><code class="name flex">
<span>def <span class="ident">initialize_parameters</span></span>(<span>self, d)</span>
</code></dt>
<dd>
<div class="desc"><p>Diese Funktion initialisiert die Parameter der Batch-Normalisierungsschicht.</p>
<p>Parameter:
d: Form der Eingabe zur BN-Schicht</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def initialize_parameters(self, d):
    &#39;&#39;&#39;
    Diese Funktion initialisiert die Parameter der Batch-Normalisierungsschicht.
    
    Parameter:
    d: Form der Eingabe zur BN-Schicht
    &#39;&#39;&#39;
    self.gamma = np.ones((d))
    self.beta = np.zeros((d))
    self.running_mean = np.zeros((d))
    self.running_var = np.zeros((d))</code></pre>
</details>
</dd>
<dt id="Neural.Network.BatchNormalization.update"><code class="name flex">
<span>def <span class="ident">update</span></span>(<span>self, learnrate, batch, k)</span>
</code></dt>
<dd>
<div class="desc"><p>Diese Funktion aktualisiert die Skalierungs- und Verschiebungsparameter basierend auf den w√§hrend der R√ºckw√§rtspropagation
berechneten Gradienten.</p>
<p>Parameter:
learnrate: Lernrate
batch: Batch-Gr√∂√üe (Anzahl der Proben im Batch)
k: Iterationsnummer</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def update(self, learnrate, batch, k):
    &#39;&#39;&#39;
    Diese Funktion aktualisiert die Skalierungs- und Verschiebungsparameter basierend auf den w√§hrend der R√ºckw√§rtspropagation 
    berechneten Gradienten.
    
    Parameter:
    learnrate: Lernrate
    batch: Batch-Gr√∂√üe (Anzahl der Proben im Batch)
    k: Iterationsnummer
    &#39;&#39;&#39;
    self.gamma -= self.dgamma*(learnrate/batch)
    self.beta -= self.dbeta*(learnrate/batch)</code></pre>
</details>
</dd>
</dl>
</dd>
<dt id="Neural.Network.Network"><code class="flex name class">
<span>class <span class="ident">Network</span></span>
<span>(</span><span>layers=None)</span>
</code></dt>
<dd>
<div class="desc"><p>Erstellt ein sequentielles CNN-Modell.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>layers</code></strong> :&ensp;<code>list</code>, optional</dt>
<dd>Eine Liste von Schichten, die dem Modell hinzugef√ºgt werden sollen.
Wenn None, wird eine leere Liste erstellt. Der Standardwert ist None.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class Network:
    def __init__(self, layers=None):
        &#39;&#39;&#39;
        Erstellt ein sequentielles CNN-Modell.

        Parameters
        ----------
        layers : list, optional
            Eine Liste von Schichten, die dem Modell hinzugef√ºgt werden sollen.
            Wenn None, wird eine leere Liste erstellt. Der Standardwert ist None.
        &#39;&#39;&#39;
        if layers is None:
            self.layers = []
        else:
            self.layers = layers
        self.network_architecture_called = False # Ein Attribut, das angibt, ob die Architektur des Modells berechnet wurde

    def add(self, layer):
        &#39;&#39;&#39;
        F√ºgt eine Schicht zum Modell hinzu.

        Parameters
        ----------
        layer : object
            Ein Objekt, das eine Schicht repr√§sentiert, z.B. Conv2D, Dense, etc.
        &#39;&#39;&#39;
        # F√ºgt die Schicht zur Liste der Schichten hinzu
        self.layers.append(layer)

    def Input(self, input_shape):
        &#39;&#39;&#39;
        Definiert die Eingabeform des Modells.

        Parameters
        ----------
        input_shape : tuple
            Ein Tupel, das die Form der Eingabedaten angibt, z.B. (3, 32, 32) f√ºr RGB-Bilder mit 32x32 Pixeln.
        &#39;&#39;&#39;
        self.d = input_shape # Die Dimension der Eingabe
        self.architecture = [self.d] # Eine Liste, die die Ausgabeform jeder Schicht speichert
        self.layer_name = [&#34;Input&#34;] # Eine Liste, die die Namen jeder Schicht speichert

    def network_architecture(self):
        &#39;&#39;&#39;
        Berechnet die Architektur des Modells basierend auf den hinzugef√ºgten Schichten.
        &#39;&#39;&#39;
        for layer in self.layers: # Iteriert √ºber jede Schicht in der Liste
            if isinstance(layer, Conv2D): # Wenn die Schicht eine Conv2D-Schicht ist
                if layer.input_shape_x is not None: # Wenn die Schicht eine Eingabeform definiert hat
                    self.Input(layer.input_shape_x) # Ruft die Input-Methode mit dieser Form auf
                layer.get_dimensions(self.architecture[-1]) # Berechnet die Ausgabeform der Schicht basierend auf der vorherigen Schicht
                self.architecture.append(layer.output_shape) # F√ºgt die Ausgabeform zur Architekturliste hinzu
                self.layer_name.append(layer.__class__.__name__) # F√ºgt den Namen der Schicht zur Namensliste hinzu
            elif isinstance(layer, (Flatten, Pooling2D)): # Wenn die Schicht eine Flatten- oder Pooling2D-Schicht ist
                layer.get_dimensions(self.architecture[-1]) # Berechnet die Ausgabeform der Schicht basierend auf der vorherigen Schicht
                self.architecture.append(layer.output_shape) # F√ºgt die Ausgabeform zur Architekturliste hinzu
                self.layer_name.append(layer.__class__.__name__) # F√ºgt den Namen der Schicht zur Namensliste hinzu
            elif isinstance(layer, Dense): # Wenn die Schicht eine Dense-Schicht ist
                self.architecture.append(layer.neurons) # F√ºgt die Anzahl der Neuronen zur Architekturliste hinzu
                self.layer_name.append(layer.__class__.__name__) # F√ºgt den Namen der Schicht zur Namensliste hinzu
            else: # Wenn die Schicht eine andere Art von Schicht ist
                self.architecture.append(self.architecture[-1]) # F√ºgt die gleiche Ausgabeform wie die vorherige Schicht zur Architekturliste hinzu
                self.layer_name.append(layer.__class__.__name__) # F√ºgt den Namen der Schicht zur Namensliste hinzu

        self.layers = list(filter(None, self.layers)) # Entfernt alle None-Elemente aus der Schichtenliste
        try:
            idx = self.layer_name.index(&#34;NoneType&#34;) # Sucht nach dem Index eines NoneType-Elements in der Namensliste
            del self.layer_name[idx] # L√∂scht das Element an diesem Index aus der Namensliste
            del self.architecture[idx] # L√∂scht das Element an diesem Index aus der Architekturliste
        except:
            pass # Wenn kein NoneType-Element gefunden wurde, tue nichts

    def summary(self):
        &#39;&#39;&#39;
        Zeigt eine Zusammenfassung des Modells an, einschlie√ülich der Schichttypen, der Ausgabeformen und der Anzahl der Parameter.

        Parameters
        ----------
        self : object
            Eine Instanz der Klasse Network.

        Returns
        -------
        None
            Die Methode gibt nichts zur√ºck, sondern druckt die Zusammenfassung auf dem Bildschirm aus.
        &#39;&#39;&#39;
        if self.network_architecture_called==False: # Wenn die Architektur des Modells noch nicht berechnet wurde
            self.network_architecture() # Ruft die Methode network_architecture auf, um die Architektur zu berechnen
            self.network_architecture_called = True # Setzt das Attribut network_architecture_called auf True
        len_assigned = [45, 26, 15] # Eine Liste von L√§ngen, die f√ºr die Spalten der Zusammenfassung zugewiesen werden
        count = {&#39;Dense&#39;: 1, &#39;Activation&#39;: 1, &#39;Input&#39;: 1,
                &#39;BatchNormalization&#39;: 1, &#39;Dropout&#39;: 1, &#39;Conv2D&#39;: 1,
                &#39;Pooling2D&#39;: 1, &#39;Flatten&#39;: 1} # Ein W√∂rterbuch, das die Anzahl jeder Schichtart speichert

        col_names = [&#39;Layer (type)&#39;, &#39;Output Shape&#39;, &#39;# of Parameters&#39;] # Eine Liste von Spaltennamen f√ºr die Zusammenfassung

        print(&#34;Model: CNN&#34;) # Druckt den Namen des Modells
        print(&#39;-&#39;*sum(len_assigned)) # Druckt eine Trennlinie
        
        text = &#39;&#39; # Initialisiert einen leeren Text
        for i in range(3): # Iteriert √ºber die drei Spalten
            text += col_names[i] + &#39; &#39;*(len_assigned[i]-len(col_names[i])) # F√ºgt den Spaltennamen und die erforderlichen Leerzeichen zum Text hinzu
        print(text) # Druckt den Text

        print(&#39;=&#39;*sum(len_assigned)) # Druckt eine Trennlinie

        total_params = 0 # Initialisiert die Gesamtzahl der Parameter auf 0
        trainable_params = 0 # Initialisiert die Anzahl der trainierbaren Parameter auf 0
        non_trainable_params = 0 # Initialisiert die Anzahl der nicht trainierbaren Parameter auf 0

        for i in range(len(self.layer_name)): # Iteriert √ºber jede Schicht in der Namensliste
            # layer name
            layer_name = self.layer_name[i] # Speichert den Namen der Schicht
            name = layer_name.lower() + &#39;_&#39; + str(count[layer_name]) + &#39; &#39; + &#39;(&#39; + layer_name + &#39;)&#39; # Erstellt einen eindeutigen Namen f√ºr die Schicht mit ihrer Nummer und ihrem Typ
            count[layer_name] += 1 # Erh√∂ht die Anzahl dieser Schichtart um 1

            # output shape
            try: # Versucht, die Ausgabeform der Schicht als Tupel zu erstellen
                out = &#39;(None, &#39; # Beginnt das Tupel mit None f√ºr die Batch-Dimension
                for n in range(len(self.architecture[i])-1): # Iteriert √ºber die restlichen Dimensionen au√üer der letzten
                    out += str(self.architecture[i][n]) + &#39;, &#39; # F√ºgt die Dimension und ein Komma zum Tupel hinzu
                out += str(self.architecture[i][-1]) + &#39;)&#39; # F√ºgt die letzte Dimension und eine schlie√üende Klammer zum Tupel hinzu
            except: # Wenn die Ausgabeform keine Tupel ist
                out = &#39;(None, &#39; + str(self.architecture[i]) + &#39;)&#39; # Erstellt die Ausgabeform als Tupel mit nur einer Dimension

            # number of params
            if layer_name==&#39;Dense&#39;: # Wenn die Schicht eine Dense-Schicht ist
                h0 = self.architecture[i-1] # Speichert die Anzahl der Eingangsneuronen
                h1 = self.architecture[i] # Speichert die Anzahl der Ausgangsneuronen
                if self.layers[i-1].use_bias: # Wenn die Schicht einen Bias-Vektor verwendet
                    params = h0*h1 + h1 # Berechnet die Anzahl der Parameter als das Produkt der Neuronen plus die Anzahl der Ausgangsneuronen
                else: # Wenn die Schicht keinen Bias-Vektor verwendet
                    params = h0*h1 # Berechnet die Anzahl der Parameter als das Produkt der Neuronen
                total_params += params # Addiert die Anzahl der Parameter zur Gesamtzahl hinzu
                trainable_params += params # Addiert die Anzahl der Parameter zur Anzahl der trainierbaren Parameter hinzu
            elif layer_name==&#39;BatchNormalization&#39;: # Wenn die Schicht eine BatchNormalization-Schicht ist
                h = self.architecture[i] # Speichert die Anzahl der Merkmale
                params = 4*h # Berechnet die Anzahl der Parameter als das Vierfache der Merkmale
                trainable_params += 2*h # Addiert die H√§lfte der Parameter zur Anzahl der trainierbaren Parameter hinzu
                non_trainable_params += 2*h # Addiert die H√§lfte der Parameter zur Anzahl der nicht trainierbaren Parameter hinzu
                total_params += params # Addiert die Anzahl der Parameter zur Gesamtzahl hinzu
            elif layer_name==&#39;Conv2D&#39;: # Wenn die Schicht eine Conv2D-Schicht ist
                layer = self.layers[i-1] # Speichert die Schicht als ein Objekt
                if layer.use_bias: # Wenn die Schicht einen Bias-Vektor verwendet
                    add_b = 1 # Speichert eine zus√§tzliche Einheit f√ºr den Bias
                else: # Wenn die Schicht keinen Bias-Vektor verwendet
                    add_b = 0 # Speichert keine zus√§tzliche Einheit f√ºr den Bias
                params = ((layer.inputC * layer.kernelH * layer.kernelW) + add_b) * layer.F # Berechnet die Anzahl der Parameter als das Produkt der Eingangskan√§le, der Kernelh√∂he, der Kernelbreite und der Anzahl der Filter plus die zus√§tzliche Einheit f√ºr den Bias
                trainable_params += params # Addiert die Anzahl der Parameter zur Anzahl der trainierbaren Parameter hinzu
                total_params += params # Addiert die Anzahl der Parameter zur Gesamtzahl hinzu
            else: # Wenn die Schicht eine andere Art von Schicht ist
                # Pooling, Dropout, Flatten, Input
                params = 0 # Speichert die Anzahl der Parameter als 0
            names = [name, out, str(params)] # Erstellt eine Liste mit dem Namen, der Ausgabeform und der Anzahl der Parameter der Schicht

            # print this row
            text = &#39;&#39; # Initialisiert einen leeren Text
            for j in range(3): # Iteriert √ºber die drei Spalten
                text += names[j] + &#39; &#39;*(len_assigned[j]-len(names[j])) # F√ºgt den Namen, die Ausgabeform oder die Anzahl der Parameter und die erforderlichen Leerzeichen zum Text hinzu
            print(text) # Druckt den Text
            if i!=(len(self.layer_name)-1): # Wenn dies nicht die letzte Schicht ist
                print(&#39;-&#39;*sum(len_assigned)) # Druckt eine Trennlinie
            else: # Wenn dies die letzte Schicht ist
                print(&#39;=&#39;*sum(len_assigned)) # Druckt eine Trennlinie

        print(&#34;Total params:&#34;, total_params) # Druckt die Gesamtzahl der Parameter
        print(&#34;Trainable params:&#34;, trainable_params) # Druckt die Anzahl der trainierbaren Parameter
        print(&#34;Non-trainable params:&#34;, non_trainable_params) # Druckt die Anzahl der nicht trainierbaren Parameter
        print(&#39;-&#39;*sum(len_assigned)) # Druckt eine Trennlinie
    
    def compile(self, cost_type, optimizer_type):
        &#39;&#39;&#39;
        Kompiliert das Modell mit einer Kostenfunktion und einem Optimierer.

        Parameters
        ----------
        self : object
            Eine Instanz der Klasse Network.
        cost_type : str
            Der Name der Kostenfunktion, die f√ºr das Modell verwendet werden soll, z.B. &#34;cross-entropy&#34; oder &#34;mse&#34;.
        optimizer_type : str
            Der Name des Optimierers, der f√ºr das Modell verwendet werden soll, z.B. &#34;sgd&#34; oder &#34;adam&#34;.

        Returns
        -------
        None
            Die Methode gibt nichts zur√ºck, sondern setzt die Attribute cost, cost_type und optimizer_type der Instanz.
        &#39;&#39;&#39;
        self.cost = Cost(cost_type) # Erstellt ein Objekt der Klasse Cost mit der angegebenen Kostenfunktion
        self.cost_type = cost_type # Speichert den Namen der Kostenfunktion als Attribut
        self.optimizer_type = optimizer_type # Speichert den Namen des Optimierers als Attribut

    def initialize_parameters(self):
        &#39;&#39;&#39;
        Initialisiert die Parameter des Modells basierend auf den hinzugef√ºgten Schichten.

        Parameters
        ----------
        self : object
            Eine Instanz der Klasse Network.

        Returns
        -------
        None
            Die Methode gibt nichts zur√ºck, sondern setzt die Parameter der Schichten als Attribute der Instanz.
        &#39;&#39;&#39;
        if not self.network_architecture_called: # Wenn die Architektur des Modells noch nicht berechnet wurde
            self.network_architecture() # Ruft die Methode network_architecture auf, um die Architektur zu berechnen
            self.network_architecture_called = True # Setzt das Attribut network_architecture_called auf True
        for i, layer in enumerate(self.layers): # Iteriert √ºber jede Schicht in der Liste der Schichten
            if isinstance(layer, (Dense, Conv2D)): # Wenn die Schicht eine Dense- oder Conv2D-Schicht ist
                #print(&#34;Layer: &#34;, layer.__class__.__name__, &#34; input: &#34;, self.architecture[i])
                layer.initialize_parameters(self.architecture[i], self.optimizer_type) # Ruft die Methode initialize_parameters der Schicht auf, um die Parameter zu initialisieren
            elif isinstance(layer, BatchNormalization): # Wenn die Schicht eine BatchNormalization-Schicht ist
                layer.initialize_parameters(self.architecture[i]) # Ruft die Methode initialize_parameters der Schicht auf, um die Parameter zu initialisieren


    
    def fit(self, X, y, epochs=10, batch_size=5, learnrate=1, X_val=None, y_val=None, verbose=1, learnrate_decay=None, **kwargs):
        &#39;&#39;&#39;
        Trainiert das Modell mit den gegebenen Trainingsdaten und optionalen Validierungsdaten.

        Parameters
        ----------
        self : object
            Eine Instanz der Klasse Network.
        X : array-like
            Die Eingabedaten f√ºr das Training, z.B. ein Numpy-Array oder eine Liste von Arrays.
        y : array-like
            Die Ausgabedaten f√ºr das Training, z.B. ein Numpy-Array oder eine Liste von Arrays.
        epochs : int, optional
            Die Anzahl der Epochen, die das Modell trainieren soll. Der Standardwert ist 10.
        batch_size : int, optional
            Die Gr√∂√üe der Minibatches, die f√ºr das Training verwendet werden sollen. Der Standardwert ist 5.
        learnrate : float, optional
            Die Lernrate, die f√ºr den Optimierer verwendet werden soll. Der Standardwert ist 1.
        X_val : array-like, optional
            Die Eingabedaten f√ºr die Validierung, z.B. ein Numpy-Array oder eine Liste von Arrays. Wenn None, wird keine Validierung durchgef√ºhrt. Der Standardwert ist None.
        y_val : array-like, optional
            Die Ausgabedaten f√ºr die Validierung, z.B. ein Numpy-Array oder eine Liste von Arrays. Wenn None, wird keine Validierung durchgef√ºhrt. Der Standardwert ist None.
        verbose : int, optional
            Ein Schalter, der angibt, ob die Trainings- und Validierungsergebnisse nach jeder Epoche gedruckt werden sollen. Wenn 1, werden die Ergebnisse gedruckt. Wenn 0, werden die Ergebnisse nicht gedruckt. Der Standardwert ist 1.
        learnrate_decay : function, optional
            Eine Funktion, die die Lernrate nach jeder Iteration anpasst. Wenn None, wird keine Lernratenanpassung durchgef√ºhrt. Der Standardwert ist None.
        **kwargs : dict, optional
            Zus√§tzliche Schl√ºsselwortargumente, die an die Funktion learnrate_decay √ºbergeben werden sollen.

        Returns
        -------
        None
            Die Methode gibt nichts zur√ºck, sondern aktualisiert die Parameter des Modells und speichert die Trainings- und Validierungshistorie als Attribute der Instanz.
        &#39;&#39;&#39;
        self.history = {&#39;Training Loss&#39;: [],&#39;Validation Loss&#39;: [], &#39;Training Accuracy&#39;: [],  &#39;Validation Accuracy&#39;: []} # Erstellt ein W√∂rterbuch, das die Trainings- und Validierungshistorie speichert
        iterations = 0 # Initialisiert die Anzahl der Iterationen auf 0
        self.batch = batch_size # Speichert die Gr√∂√üe der Minibatches als Attribut
        self.initialize_parameters() # Ruft die Methode initialize_parameters auf, um die Parameter des Modells zu initialisieren
        total_num_batches = np.ceil(len(X)/batch_size) # Berechnet die Gesamtzahl der Minibatches

        for epoch in range(epochs): # Iteriert √ºber jede Epoche
            cost_train = 0 # Initialisiert die Trainingskosten auf 0
            num_batches = 0 # Initialisiert die Anzahl der Minibatches auf 0
            y_pred_train = [] # Initialisiert eine Liste, die die Vorhersagen des Modells f√ºr die Trainingsdaten speichert
            y_train = [] # Initialisiert eine Liste, die die tats√§chlichen Ausgaben f√ºr die Trainingsdaten speichert

            print(f&#39;\nEpoch: {epoch+1}/{epochs}&#39;) # Druckt die aktuelle Epoche

            for i in tqdm(range(0, len(X), batch_size)): # Iteriert √ºber jede Minibatch
                X_batch = X[i:i+batch_size] # Extrahiert die Eingabedaten f√ºr die Minibatch
                y_batch = y[i:i+batch_size] # Extrahiert die Ausgabedaten f√ºr die Minibatch

                Z = X_batch.copy() # Erstellt eine Kopie der Eingabedaten

                # feed-forward
                for layer in self.layers: # Iteriert √ºber jede Schicht im Modell
                    Z = layer.forward(Z) # Ruft die Methode forward der Schicht auf, um die Ausgabe der Schicht zu berechnen

                # calculating training accuracy
                if self.cost_type==&#39;cross-entropy&#39;: # Wenn die Kostenfunktion die Kreuzentropie ist
                    y_pred_train += np.argmax(Z, axis=1).tolist() # F√ºgt die Vorhersagen des Modells f√ºr die Minibatch zur Liste der Vorhersagen hinzu
                    y_train += np.argmax(y_batch, axis=1).tolist() # F√ºgt die tats√§chlichen Ausgaben f√ºr die Minibatch zur Liste der Ausgaben hinzu

                # calculating the loss
                cost_train += self.cost.get_cost(Z, y_batch) / self.batch # Berechnet die Kosten f√ºr die Minibatch und addiert sie zu den Trainingskosten

                # calculating dL/daL (last layer backprop error)
                dZ = self.cost.get_d_cost(Z, y_batch) # Berechnet den Fehler der letzten Schicht
                # backpropagation
                for layer in self.layers[::-1]: # Iteriert √ºber jede Schicht im Modell in umgekehrter Reihenfolge
                    dZ = layer.backpropagation(dZ) # Ruft die Methode backpropagation der Schicht auf, um den Fehler an die vorherige Schicht weiterzugeben

                # Parameters update
                for layer in self.layers: # Iteriert √ºber jede Schicht im Modell
                    if isinstance(layer, (Dense, BatchNormalization, Conv2D)): # Wenn die Schicht eine Dense-, BatchNormalization- oder Conv2D-Schicht ist
                        layer.update(learnrate, self.batch, iterations) # Ruft die Methode update der Schicht auf, um die Parameter der Schicht zu aktualisieren

                # Learning rate decay
                if learnrate_decay is not None: # Wenn eine Lernratenanpassungsfunktion angegeben ist
                    learnrate = learnrate_decay(iterations, **kwargs) # Ruft die Funktion learnrate_decay auf, um die Lernrate anzupassen

                num_batches += 1 # Erh√∂ht die Anzahl der Minibatches um 1
                iterations += 1 # Erh√∂ht die Anzahl der Iterationen um 1

            cost_train /= num_batches # Berechnet den Durchschnitt der Trainingskosten f√ºr die Epoche

            # printing purpose only (Training Accuracy, Validation loss and accuracy)

            text  = f&#39;Training Loss: {round(cost_train, 4)} - &#39; # Erstellt einen Text, der die Trainingskosten enth√§lt
            self.history[&#39;Training Loss&#39;].append(cost_train) # F√ºgt die Trainingskosten zur Historie hinzu

            # training accuracy

            if self.cost_type==&#39;cross-entropy&#39;: # Wenn die Kostenfunktion die Kreuzentropie ist
                accuracy_train = np.sum(np.array(y_pred_train) == np.array(y_train)) / len(y_train) # Berechnet die Trainingsgenauigkeit f√ºr die Epoche
                text += f&#39;Training Accuracy: {round(accuracy_train, 4)}&#39; # F√ºgt die Trainingsgenauigkeit zum Text hinzu
                self.history[&#39;Training Accuracy&#39;].append(accuracy_train) # F√ºgt die Trainingsgenauigkeit zur Historie hinzu
            else: # Wenn die Kostenfunktion eine andere ist
                text += f&#39;Training Accuracy: {round(cost_train, 4)}&#39; # F√ºgt die Trainingskosten als Genauigkeit zum Text hinzu
                self.history[&#39;Training Accuracy&#39;].append(cost_train) # F√ºgt die Trainingskosten als Genauigkeit zur Historie hinzu

            if X_val is not None: # Wenn Validierungsdaten angegeben sind
                cost_val, accuracy_val = self.evaluate(X_val, y_val, batch_size) # Ruft die Methode evaluate auf, um die Validierungskosten und -genauigkeit zu berechnen
                text += f&#39; - Validation Loss: {round(cost_val, 4)} - &#39; # F√ºgt die Validierungskosten zum Text hinzu
                self.history[&#39;Validation Loss&#39;].append(cost_val) # F√ºgt die Validierungskosten zur Historie hinzu
                text += f&#39;Validation Accuracy: {round(accuracy_val, 4)}&#39; # F√ºgt die Validierungsgenauigkeit zum Text hinzu
                self.history[&#39;Validation Accuracy&#39;].append(accuracy_val) # F√ºgt die Validierungsgenauigkeit zur Historie hinzu

            if verbose:
                    print(text)
            else:
                print()
    
    def evaluate(self, X, y, batch_size=None):
        &#39;&#39;&#39;
        Bewertet das Modell mit den gegebenen Testdaten.

        Parameters
        ----------
        self : object
            Eine Instanz der Klasse Network.
        X : array-like
            Die Eingabedaten f√ºr den Test, z.B. ein Numpy-Array oder eine Liste von Arrays.
        y : array-like
            Die Ausgabedaten f√ºr den Test, z.B. ein Numpy-Array oder eine Liste von Arrays.
        batch_size : int, optional
            Die Gr√∂√üe der Minibatches, die f√ºr den Test verwendet werden sollen. Wenn None, wird die L√§nge von X verwendet. Der Standardwert ist None.

        Returns
        -------
        cost : float
            Die Kosten des Modells f√ºr die Testdaten, berechnet mit der Kostenfunktion des Modells.
        accuracy : float
            Die Genauigkeit des Modells f√ºr die Testdaten, berechnet als der Anteil der korrekten Vorhersagen.
        &#39;&#39;&#39;
        if batch_size is None: # Wenn keine Batch-Gr√∂√üe angegeben ist
            batch_size = len(X) # Verwendet die L√§nge von X als Batch-Gr√∂√üe

        cost = 0 # Initialisiert die Kosten auf 0
        correct = 0 # Initialisiert die Anzahl der korrekten Vorhersagen auf 0
        num_batches = 0 # Initialisiert die Anzahl der Minibatches auf 0
        utility = Utility() # Erstellt ein Objekt der Klasse Utility
        Y_1hot, _ = utility.onehot(y) # Wandelt die Ausgabedaten in One-Hot-Vektoren um

        for i in range(0, len(X), batch_size): # Iteriert √ºber jede Minibatch
            X_batch = X[i:i+batch_size] # Extrahiert die Eingabedaten f√ºr die Minibatch
            y_batch = y[i:i+batch_size] # Extrahiert die Ausgabedaten f√ºr die Minibatch
            Y_1hot_batch = Y_1hot[i:i+batch_size] # Extrahiert die One-Hot-Vektoren f√ºr die Minibatch
            Z = X_batch.copy() # Erstellt eine Kopie der Eingabedaten
            for layer in self.layers: # Iteriert √ºber jede Schicht im Modell
                if layer.__class__.__name__==&#39;BatchNormalization&#39;: # Wenn die Schicht eine BatchNormalization-Schicht ist
                    Z = layer.forward(Z, mode=&#39;test&#39;) # Ruft die Methode forward der Schicht auf, um die Ausgabe der Schicht im Testmodus zu berechnen
                else: # Wenn die Schicht eine andere Art von Schicht ist
                    Z = layer.forward(Z) # Ruft die Methode forward der Schicht auf, um die Ausgabe der Schicht zu berechnen
            if self.cost_type==&#39;cross-entropy&#39;: # Wenn die Kostenfunktion die Kreuzentropie ist
                cost += self.cost.get_cost(Z, Y_1hot_batch) / len(y_batch) # Berechnet die Kosten f√ºr die Minibatch und addiert sie zu den Gesamtkosten
                y_pred = np.argmax(Z, axis=1).tolist() # Berechnet die Vorhersagen des Modells f√ºr die Minibatch
                correct += np.sum(y_pred == y_batch) # Z√§hlt die Anzahl der korrekten Vorhersagen f√ºr die Minibatch
            else: # Wenn die Kostenfunktion eine andere ist
                cost += self.cost.get_cost(Z, y_batch) / len(y_batch) # Berechnet die Kosten f√ºr die Minibatch und addiert sie zu den Gesamtkosten

            num_batches += 1 # Erh√∂ht die Anzahl der Minibatches um 1

        if self.cost_type==&#39;cross-entropy&#39;: # Wenn die Kostenfunktion die Kreuzentropie ist
            accuracy = correct / len(y) # Berechnet die Genauigkeit des Modells f√ºr die Testdaten
            cost /= num_batches # Berechnet den Durchschnitt der Kosten f√ºr die Testdaten
            return cost, accuracy # Gibt die Kosten und die Genauigkeit zur√ºck
        else: # Wenn die Kostenfunktion eine andere ist
            cost /= num_batches # Berechnet den Durchschnitt der Kosten f√ºr die Testdaten
            return cost, cost # Gibt die Kosten zweimal zur√ºck

    def loss_plot(self):
        &#39;&#39;&#39;
        Zeigt einen Plot der Trainings- und Validierungskosten pro Epoche an.

        Parameters
        ----------
        self : object
            Eine Instanz der Klasse Network.

        Returns
        -------
        None
            Die Methode gibt nichts zur√ºck, sondern zeigt den Plot auf dem Bildschirm an.
        &#39;&#39;&#39;
        plt.plot(self.history[&#39;Training Loss&#39;], &#39;k&#39;) # Plottet die Trainingskosten in schwarz
        if len(self.history[&#39;Validation Loss&#39;])&gt;0: # Wenn es Validierungskosten gibt
            plt.plot(self.history[&#39;Validation Loss&#39;], &#39;r&#39;) # Plottet die Validierungskosten in rot
            plt.legend([&#39;Train&#39;, &#39;Validation&#39;], loc=&#39;upper right&#39;) # F√ºgt eine Legende mit den Namen der Kurven hinzu
            plt.title(&#39;Model Loss&#39;) # F√ºgt einen Titel f√ºr den Plot hinzu
        else: # Wenn es keine Validierungskosten gibt
            plt.title(&#39;Training Loss&#39;) # F√ºgt einen Titel f√ºr den Plot hinzu
        plt.ylabel(&#39;Loss&#39;) # F√ºgt eine Beschriftung f√ºr die y-Achse hinzu
        plt.xlabel(&#39;Epoch&#39;) # F√ºgt eine Beschriftung f√ºr die x-Achse hinzu
        plt.show() # Zeigt den Plot an

    def accuracy_plot(self):
        &#39;&#39;&#39;
        Zeigt einen Plot der Trainings- und Validierungsgenauigkeit pro Epoche an.

        Parameters
        ----------
        self : object
            Eine Instanz der Klasse Network.

        Returns
        -------
        None
            Die Methode gibt nichts zur√ºck, sondern zeigt den Plot auf dem Bildschirm an.
        &#39;&#39;&#39;
        plt.plot(self.history[&#39;Training Accuracy&#39;], &#39;k&#39;) # Plottet die Trainingsgenauigkeit in schwarz
        if len(self.history[&#39;Validation Accuracy&#39;])&gt;0: # Wenn es Validierungsgenauigkeit gibt
            plt.plot(self.history[&#39;Validation Accuracy&#39;], &#39;r&#39;) # Plottet die Validierungsgenauigkeit in rot
            plt.legend([&#39;Train&#39;, &#39;Validation&#39;], loc=&#39;lower right&#39;) # F√ºgt eine Legende mit den Namen der Kurven hinzu
            plt.title(&#39;Model Accuracy&#39;) # F√ºgt einen Titel f√ºr den Plot hinzu
        else: # Wenn es keine Validierungsgenauigkeit gibt
            plt.title(&#39;Training Accuracy&#39;) # F√ºgt einen Titel f√ºr den Plot hinzu
        plt.ylabel(&#39;Accuracy&#39;) # F√ºgt eine Beschriftung f√ºr die y-Achse hinzu
        plt.xlabel(&#39;Epoch&#39;) # F√ºgt eine Beschriftung f√ºr die x-Achse hinzu
        plt.show() # Zeigt den Plot an

    def predict(self, X, batch_size=None):
        &#39;&#39;&#39;
        Erzeugt Vorhersagen des Modells f√ºr die gegebenen Eingabedaten.

        Parameters
        ----------
        self : object
            Eine Instanz der Klasse Network.
        X : array-like
            Die Eingabedaten, f√ºr die das Modell Vorhersagen machen soll, z.B. ein Numpy-Array oder eine Liste von Arrays.
        batch_size : int, optional
            Die Gr√∂√üe der Minibatches, die f√ºr die Vorhersage verwendet werden sollen. Wenn None, wird die L√§nge von X verwendet. Der Standardwert ist None.

        Returns
        -------
        y_pred : array-like
            Die Vorhersagen des Modells f√ºr die Eingabedaten, z.B. ein Numpy-Array oder eine Liste von Arrays.
        &#39;&#39;&#39;
        if batch_size==None: # Wenn keine Batch-Gr√∂√üe angegeben ist
            batch_size = len(X) # Verwendet die L√§nge von X als Batch-Gr√∂√üe

        for i in range(0, len(X), batch_size): # Iteriert √ºber jede Minibatch
            X_batch = X[i:i+batch_size] # Extrahiert die Eingabedaten f√ºr die Minibatch
            Z = X_batch.copy() # Erstellt eine Kopie der Eingabedaten
            for layer in self.layers: # Iteriert √ºber jede Schicht im Modell
                if layer.__class__.__name__==&#39;BatchNormalization&#39;: # Wenn die Schicht eine BatchNormalization-Schicht ist
                    Z = layer.forward(Z, mode=&#39;test&#39;) # Ruft die Methode forward der Schicht auf, um die Ausgabe der Schicht im Testmodus zu berechnen
                else: # Wenn die Schicht eine andere Art von Schicht ist
                    Z = layer.forward(Z) # Ruft die Methode forward der Schicht auf, um die Ausgabe der Schicht zu berechnen
            if i==0: # Wenn dies die erste Minibatch ist
                if self.cost_type==&#39;cross-entropy&#39;: # Wenn die Kostenfunktion die Kreuzentropie ist
                    y_pred = np.argmax(Z, axis=1).tolist() # Berechnet die Vorhersagen des Modells f√ºr die Minibatch als eine Liste von Indizes
                else: # Wenn die Kostenfunktion eine andere ist
                    y_pred = Z # Speichert die Ausgabe des Modells f√ºr die Minibatch als ein Array
            else: # Wenn dies nicht die erste Minibatch ist
                if self.cost_type==&#39;cross-entropy&#39;: # Wenn die Kostenfunktion die Kreuzentropie ist
                    y_pred += np.argmax(Z, axis=1).tolist() # F√ºgt die Vorhersagen des Modells f√ºr die Minibatch zur Liste der Vorhersagen hinzu
                else: # Wenn die Kostenfunktion eine andere ist
                    y_pred = np.vstack((y_pred, Z)) # Stapelt die Ausgabe des Modells f√ºr die Minibatch unter der bisherigen Ausgabe

        return np.array(y_pred) # Gibt die Vorhersagen des Modells f√ºr die Eingabedaten als ein Array zur√ºck</code></pre>
</details>
<h3>Methods</h3>
<dl>
<dt id="Neural.Network.Network.Input"><code class="name flex">
<span>def <span class="ident">Input</span></span>(<span>self, input_shape)</span>
</code></dt>
<dd>
<div class="desc"><p>Definiert die Eingabeform des Modells.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>input_shape</code></strong> :&ensp;<code>tuple</code></dt>
<dd>Ein Tupel, das die Form der Eingabedaten angibt, z.B. (3, 32, 32) f√ºr RGB-Bilder mit 32x32 Pixeln.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def Input(self, input_shape):
    &#39;&#39;&#39;
    Definiert die Eingabeform des Modells.

    Parameters
    ----------
    input_shape : tuple
        Ein Tupel, das die Form der Eingabedaten angibt, z.B. (3, 32, 32) f√ºr RGB-Bilder mit 32x32 Pixeln.
    &#39;&#39;&#39;
    self.d = input_shape # Die Dimension der Eingabe
    self.architecture = [self.d] # Eine Liste, die die Ausgabeform jeder Schicht speichert
    self.layer_name = [&#34;Input&#34;] # Eine Liste, die die Namen jeder Schicht speichert</code></pre>
</details>
</dd>
<dt id="Neural.Network.Network.accuracy_plot"><code class="name flex">
<span>def <span class="ident">accuracy_plot</span></span>(<span>self)</span>
</code></dt>
<dd>
<div class="desc"><p>Zeigt einen Plot der Trainings- und Validierungsgenauigkeit pro Epoche an.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>self</code></strong> :&ensp;<code>object</code></dt>
<dd>Eine Instanz der Klasse Network.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>None</code></dt>
<dd>Die Methode gibt nichts zur√ºck, sondern zeigt den Plot auf dem Bildschirm an.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def accuracy_plot(self):
    &#39;&#39;&#39;
    Zeigt einen Plot der Trainings- und Validierungsgenauigkeit pro Epoche an.

    Parameters
    ----------
    self : object
        Eine Instanz der Klasse Network.

    Returns
    -------
    None
        Die Methode gibt nichts zur√ºck, sondern zeigt den Plot auf dem Bildschirm an.
    &#39;&#39;&#39;
    plt.plot(self.history[&#39;Training Accuracy&#39;], &#39;k&#39;) # Plottet die Trainingsgenauigkeit in schwarz
    if len(self.history[&#39;Validation Accuracy&#39;])&gt;0: # Wenn es Validierungsgenauigkeit gibt
        plt.plot(self.history[&#39;Validation Accuracy&#39;], &#39;r&#39;) # Plottet die Validierungsgenauigkeit in rot
        plt.legend([&#39;Train&#39;, &#39;Validation&#39;], loc=&#39;lower right&#39;) # F√ºgt eine Legende mit den Namen der Kurven hinzu
        plt.title(&#39;Model Accuracy&#39;) # F√ºgt einen Titel f√ºr den Plot hinzu
    else: # Wenn es keine Validierungsgenauigkeit gibt
        plt.title(&#39;Training Accuracy&#39;) # F√ºgt einen Titel f√ºr den Plot hinzu
    plt.ylabel(&#39;Accuracy&#39;) # F√ºgt eine Beschriftung f√ºr die y-Achse hinzu
    plt.xlabel(&#39;Epoch&#39;) # F√ºgt eine Beschriftung f√ºr die x-Achse hinzu
    plt.show() # Zeigt den Plot an</code></pre>
</details>
</dd>
<dt id="Neural.Network.Network.add"><code class="name flex">
<span>def <span class="ident">add</span></span>(<span>self, layer)</span>
</code></dt>
<dd>
<div class="desc"><p>F√ºgt eine Schicht zum Modell hinzu.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>layer</code></strong> :&ensp;<code>object</code></dt>
<dd>Ein Objekt, das eine Schicht repr√§sentiert, z.B. Conv2D, Dense, etc.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def add(self, layer):
    &#39;&#39;&#39;
    F√ºgt eine Schicht zum Modell hinzu.

    Parameters
    ----------
    layer : object
        Ein Objekt, das eine Schicht repr√§sentiert, z.B. Conv2D, Dense, etc.
    &#39;&#39;&#39;
    # F√ºgt die Schicht zur Liste der Schichten hinzu
    self.layers.append(layer)</code></pre>
</details>
</dd>
<dt id="Neural.Network.Network.compile"><code class="name flex">
<span>def <span class="ident">compile</span></span>(<span>self, cost_type, optimizer_type)</span>
</code></dt>
<dd>
<div class="desc"><p>Kompiliert das Modell mit einer Kostenfunktion und einem Optimierer.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>self</code></strong> :&ensp;<code>object</code></dt>
<dd>Eine Instanz der Klasse Network.</dd>
<dt><strong><code>cost_type</code></strong> :&ensp;<code>str</code></dt>
<dd>Der Name der Kostenfunktion, die f√ºr das Modell verwendet werden soll, z.B. "cross-entropy" oder "mse".</dd>
<dt><strong><code>optimizer_type</code></strong> :&ensp;<code>str</code></dt>
<dd>Der Name des Optimierers, der f√ºr das Modell verwendet werden soll, z.B. "sgd" oder "adam".</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>None</code></dt>
<dd>Die Methode gibt nichts zur√ºck, sondern setzt die Attribute cost, cost_type und optimizer_type der Instanz.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def compile(self, cost_type, optimizer_type):
    &#39;&#39;&#39;
    Kompiliert das Modell mit einer Kostenfunktion und einem Optimierer.

    Parameters
    ----------
    self : object
        Eine Instanz der Klasse Network.
    cost_type : str
        Der Name der Kostenfunktion, die f√ºr das Modell verwendet werden soll, z.B. &#34;cross-entropy&#34; oder &#34;mse&#34;.
    optimizer_type : str
        Der Name des Optimierers, der f√ºr das Modell verwendet werden soll, z.B. &#34;sgd&#34; oder &#34;adam&#34;.

    Returns
    -------
    None
        Die Methode gibt nichts zur√ºck, sondern setzt die Attribute cost, cost_type und optimizer_type der Instanz.
    &#39;&#39;&#39;
    self.cost = Cost(cost_type) # Erstellt ein Objekt der Klasse Cost mit der angegebenen Kostenfunktion
    self.cost_type = cost_type # Speichert den Namen der Kostenfunktion als Attribut
    self.optimizer_type = optimizer_type # Speichert den Namen des Optimierers als Attribut</code></pre>
</details>
</dd>
<dt id="Neural.Network.Network.evaluate"><code class="name flex">
<span>def <span class="ident">evaluate</span></span>(<span>self, X, y, batch_size=None)</span>
</code></dt>
<dd>
<div class="desc"><p>Bewertet das Modell mit den gegebenen Testdaten.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>self</code></strong> :&ensp;<code>object</code></dt>
<dd>Eine Instanz der Klasse Network.</dd>
<dt><strong><code>X</code></strong> :&ensp;<code>array-like</code></dt>
<dd>Die Eingabedaten f√ºr den Test, z.B. ein Numpy-Array oder eine Liste von Arrays.</dd>
<dt><strong><code>y</code></strong> :&ensp;<code>array-like</code></dt>
<dd>Die Ausgabedaten f√ºr den Test, z.B. ein Numpy-Array oder eine Liste von Arrays.</dd>
<dt><strong><code>batch_size</code></strong> :&ensp;<code>int</code>, optional</dt>
<dd>Die Gr√∂√üe der Minibatches, die f√ºr den Test verwendet werden sollen. Wenn None, wird die L√§nge von X verwendet. Der Standardwert ist None.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><strong><code>cost</code></strong> :&ensp;<code>float</code></dt>
<dd>Die Kosten des Modells f√ºr die Testdaten, berechnet mit der Kostenfunktion des Modells.</dd>
<dt><strong><code>accuracy</code></strong> :&ensp;<code>float</code></dt>
<dd>Die Genauigkeit des Modells f√ºr die Testdaten, berechnet als der Anteil der korrekten Vorhersagen.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def evaluate(self, X, y, batch_size=None):
    &#39;&#39;&#39;
    Bewertet das Modell mit den gegebenen Testdaten.

    Parameters
    ----------
    self : object
        Eine Instanz der Klasse Network.
    X : array-like
        Die Eingabedaten f√ºr den Test, z.B. ein Numpy-Array oder eine Liste von Arrays.
    y : array-like
        Die Ausgabedaten f√ºr den Test, z.B. ein Numpy-Array oder eine Liste von Arrays.
    batch_size : int, optional
        Die Gr√∂√üe der Minibatches, die f√ºr den Test verwendet werden sollen. Wenn None, wird die L√§nge von X verwendet. Der Standardwert ist None.

    Returns
    -------
    cost : float
        Die Kosten des Modells f√ºr die Testdaten, berechnet mit der Kostenfunktion des Modells.
    accuracy : float
        Die Genauigkeit des Modells f√ºr die Testdaten, berechnet als der Anteil der korrekten Vorhersagen.
    &#39;&#39;&#39;
    if batch_size is None: # Wenn keine Batch-Gr√∂√üe angegeben ist
        batch_size = len(X) # Verwendet die L√§nge von X als Batch-Gr√∂√üe

    cost = 0 # Initialisiert die Kosten auf 0
    correct = 0 # Initialisiert die Anzahl der korrekten Vorhersagen auf 0
    num_batches = 0 # Initialisiert die Anzahl der Minibatches auf 0
    utility = Utility() # Erstellt ein Objekt der Klasse Utility
    Y_1hot, _ = utility.onehot(y) # Wandelt die Ausgabedaten in One-Hot-Vektoren um

    for i in range(0, len(X), batch_size): # Iteriert √ºber jede Minibatch
        X_batch = X[i:i+batch_size] # Extrahiert die Eingabedaten f√ºr die Minibatch
        y_batch = y[i:i+batch_size] # Extrahiert die Ausgabedaten f√ºr die Minibatch
        Y_1hot_batch = Y_1hot[i:i+batch_size] # Extrahiert die One-Hot-Vektoren f√ºr die Minibatch
        Z = X_batch.copy() # Erstellt eine Kopie der Eingabedaten
        for layer in self.layers: # Iteriert √ºber jede Schicht im Modell
            if layer.__class__.__name__==&#39;BatchNormalization&#39;: # Wenn die Schicht eine BatchNormalization-Schicht ist
                Z = layer.forward(Z, mode=&#39;test&#39;) # Ruft die Methode forward der Schicht auf, um die Ausgabe der Schicht im Testmodus zu berechnen
            else: # Wenn die Schicht eine andere Art von Schicht ist
                Z = layer.forward(Z) # Ruft die Methode forward der Schicht auf, um die Ausgabe der Schicht zu berechnen
        if self.cost_type==&#39;cross-entropy&#39;: # Wenn die Kostenfunktion die Kreuzentropie ist
            cost += self.cost.get_cost(Z, Y_1hot_batch) / len(y_batch) # Berechnet die Kosten f√ºr die Minibatch und addiert sie zu den Gesamtkosten
            y_pred = np.argmax(Z, axis=1).tolist() # Berechnet die Vorhersagen des Modells f√ºr die Minibatch
            correct += np.sum(y_pred == y_batch) # Z√§hlt die Anzahl der korrekten Vorhersagen f√ºr die Minibatch
        else: # Wenn die Kostenfunktion eine andere ist
            cost += self.cost.get_cost(Z, y_batch) / len(y_batch) # Berechnet die Kosten f√ºr die Minibatch und addiert sie zu den Gesamtkosten

        num_batches += 1 # Erh√∂ht die Anzahl der Minibatches um 1

    if self.cost_type==&#39;cross-entropy&#39;: # Wenn die Kostenfunktion die Kreuzentropie ist
        accuracy = correct / len(y) # Berechnet die Genauigkeit des Modells f√ºr die Testdaten
        cost /= num_batches # Berechnet den Durchschnitt der Kosten f√ºr die Testdaten
        return cost, accuracy # Gibt die Kosten und die Genauigkeit zur√ºck
    else: # Wenn die Kostenfunktion eine andere ist
        cost /= num_batches # Berechnet den Durchschnitt der Kosten f√ºr die Testdaten
        return cost, cost # Gibt die Kosten zweimal zur√ºck</code></pre>
</details>
</dd>
<dt id="Neural.Network.Network.fit"><code class="name flex">
<span>def <span class="ident">fit</span></span>(<span>self, X, y, epochs=10, batch_size=5, learnrate=1, X_val=None, y_val=None, verbose=1, learnrate_decay=None, **kwargs)</span>
</code></dt>
<dd>
<div class="desc"><p>Trainiert das Modell mit den gegebenen Trainingsdaten und optionalen Validierungsdaten.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>self</code></strong> :&ensp;<code>object</code></dt>
<dd>Eine Instanz der Klasse Network.</dd>
<dt><strong><code>X</code></strong> :&ensp;<code>array-like</code></dt>
<dd>Die Eingabedaten f√ºr das Training, z.B. ein Numpy-Array oder eine Liste von Arrays.</dd>
<dt><strong><code>y</code></strong> :&ensp;<code>array-like</code></dt>
<dd>Die Ausgabedaten f√ºr das Training, z.B. ein Numpy-Array oder eine Liste von Arrays.</dd>
<dt><strong><code>epochs</code></strong> :&ensp;<code>int</code>, optional</dt>
<dd>Die Anzahl der Epochen, die das Modell trainieren soll. Der Standardwert ist 10.</dd>
<dt><strong><code>batch_size</code></strong> :&ensp;<code>int</code>, optional</dt>
<dd>Die Gr√∂√üe der Minibatches, die f√ºr das Training verwendet werden sollen. Der Standardwert ist 5.</dd>
<dt><strong><code>learnrate</code></strong> :&ensp;<code>float</code>, optional</dt>
<dd>Die Lernrate, die f√ºr den Optimierer verwendet werden soll. Der Standardwert ist 1.</dd>
<dt><strong><code>X_val</code></strong> :&ensp;<code>array-like</code>, optional</dt>
<dd>Die Eingabedaten f√ºr die Validierung, z.B. ein Numpy-Array oder eine Liste von Arrays. Wenn None, wird keine Validierung durchgef√ºhrt. Der Standardwert ist None.</dd>
<dt><strong><code>y_val</code></strong> :&ensp;<code>array-like</code>, optional</dt>
<dd>Die Ausgabedaten f√ºr die Validierung, z.B. ein Numpy-Array oder eine Liste von Arrays. Wenn None, wird keine Validierung durchgef√ºhrt. Der Standardwert ist None.</dd>
<dt><strong><code>verbose</code></strong> :&ensp;<code>int</code>, optional</dt>
<dd>Ein Schalter, der angibt, ob die Trainings- und Validierungsergebnisse nach jeder Epoche gedruckt werden sollen. Wenn 1, werden die Ergebnisse gedruckt. Wenn 0, werden die Ergebnisse nicht gedruckt. Der Standardwert ist 1.</dd>
<dt><strong><code>learnrate_decay</code></strong> :&ensp;<code>function</code>, optional</dt>
<dd>Eine Funktion, die die Lernrate nach jeder Iteration anpasst. Wenn None, wird keine Lernratenanpassung durchgef√ºhrt. Der Standardwert ist None.</dd>
<dt><strong><code>**kwargs</code></strong> :&ensp;<code>dict</code>, optional</dt>
<dd>Zus√§tzliche Schl√ºsselwortargumente, die an die Funktion learnrate_decay √ºbergeben werden sollen.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>None</code></dt>
<dd>Die Methode gibt nichts zur√ºck, sondern aktualisiert die Parameter des Modells und speichert die Trainings- und Validierungshistorie als Attribute der Instanz.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def fit(self, X, y, epochs=10, batch_size=5, learnrate=1, X_val=None, y_val=None, verbose=1, learnrate_decay=None, **kwargs):
    &#39;&#39;&#39;
    Trainiert das Modell mit den gegebenen Trainingsdaten und optionalen Validierungsdaten.

    Parameters
    ----------
    self : object
        Eine Instanz der Klasse Network.
    X : array-like
        Die Eingabedaten f√ºr das Training, z.B. ein Numpy-Array oder eine Liste von Arrays.
    y : array-like
        Die Ausgabedaten f√ºr das Training, z.B. ein Numpy-Array oder eine Liste von Arrays.
    epochs : int, optional
        Die Anzahl der Epochen, die das Modell trainieren soll. Der Standardwert ist 10.
    batch_size : int, optional
        Die Gr√∂√üe der Minibatches, die f√ºr das Training verwendet werden sollen. Der Standardwert ist 5.
    learnrate : float, optional
        Die Lernrate, die f√ºr den Optimierer verwendet werden soll. Der Standardwert ist 1.
    X_val : array-like, optional
        Die Eingabedaten f√ºr die Validierung, z.B. ein Numpy-Array oder eine Liste von Arrays. Wenn None, wird keine Validierung durchgef√ºhrt. Der Standardwert ist None.
    y_val : array-like, optional
        Die Ausgabedaten f√ºr die Validierung, z.B. ein Numpy-Array oder eine Liste von Arrays. Wenn None, wird keine Validierung durchgef√ºhrt. Der Standardwert ist None.
    verbose : int, optional
        Ein Schalter, der angibt, ob die Trainings- und Validierungsergebnisse nach jeder Epoche gedruckt werden sollen. Wenn 1, werden die Ergebnisse gedruckt. Wenn 0, werden die Ergebnisse nicht gedruckt. Der Standardwert ist 1.
    learnrate_decay : function, optional
        Eine Funktion, die die Lernrate nach jeder Iteration anpasst. Wenn None, wird keine Lernratenanpassung durchgef√ºhrt. Der Standardwert ist None.
    **kwargs : dict, optional
        Zus√§tzliche Schl√ºsselwortargumente, die an die Funktion learnrate_decay √ºbergeben werden sollen.

    Returns
    -------
    None
        Die Methode gibt nichts zur√ºck, sondern aktualisiert die Parameter des Modells und speichert die Trainings- und Validierungshistorie als Attribute der Instanz.
    &#39;&#39;&#39;
    self.history = {&#39;Training Loss&#39;: [],&#39;Validation Loss&#39;: [], &#39;Training Accuracy&#39;: [],  &#39;Validation Accuracy&#39;: []} # Erstellt ein W√∂rterbuch, das die Trainings- und Validierungshistorie speichert
    iterations = 0 # Initialisiert die Anzahl der Iterationen auf 0
    self.batch = batch_size # Speichert die Gr√∂√üe der Minibatches als Attribut
    self.initialize_parameters() # Ruft die Methode initialize_parameters auf, um die Parameter des Modells zu initialisieren
    total_num_batches = np.ceil(len(X)/batch_size) # Berechnet die Gesamtzahl der Minibatches

    for epoch in range(epochs): # Iteriert √ºber jede Epoche
        cost_train = 0 # Initialisiert die Trainingskosten auf 0
        num_batches = 0 # Initialisiert die Anzahl der Minibatches auf 0
        y_pred_train = [] # Initialisiert eine Liste, die die Vorhersagen des Modells f√ºr die Trainingsdaten speichert
        y_train = [] # Initialisiert eine Liste, die die tats√§chlichen Ausgaben f√ºr die Trainingsdaten speichert

        print(f&#39;\nEpoch: {epoch+1}/{epochs}&#39;) # Druckt die aktuelle Epoche

        for i in tqdm(range(0, len(X), batch_size)): # Iteriert √ºber jede Minibatch
            X_batch = X[i:i+batch_size] # Extrahiert die Eingabedaten f√ºr die Minibatch
            y_batch = y[i:i+batch_size] # Extrahiert die Ausgabedaten f√ºr die Minibatch

            Z = X_batch.copy() # Erstellt eine Kopie der Eingabedaten

            # feed-forward
            for layer in self.layers: # Iteriert √ºber jede Schicht im Modell
                Z = layer.forward(Z) # Ruft die Methode forward der Schicht auf, um die Ausgabe der Schicht zu berechnen

            # calculating training accuracy
            if self.cost_type==&#39;cross-entropy&#39;: # Wenn die Kostenfunktion die Kreuzentropie ist
                y_pred_train += np.argmax(Z, axis=1).tolist() # F√ºgt die Vorhersagen des Modells f√ºr die Minibatch zur Liste der Vorhersagen hinzu
                y_train += np.argmax(y_batch, axis=1).tolist() # F√ºgt die tats√§chlichen Ausgaben f√ºr die Minibatch zur Liste der Ausgaben hinzu

            # calculating the loss
            cost_train += self.cost.get_cost(Z, y_batch) / self.batch # Berechnet die Kosten f√ºr die Minibatch und addiert sie zu den Trainingskosten

            # calculating dL/daL (last layer backprop error)
            dZ = self.cost.get_d_cost(Z, y_batch) # Berechnet den Fehler der letzten Schicht
            # backpropagation
            for layer in self.layers[::-1]: # Iteriert √ºber jede Schicht im Modell in umgekehrter Reihenfolge
                dZ = layer.backpropagation(dZ) # Ruft die Methode backpropagation der Schicht auf, um den Fehler an die vorherige Schicht weiterzugeben

            # Parameters update
            for layer in self.layers: # Iteriert √ºber jede Schicht im Modell
                if isinstance(layer, (Dense, BatchNormalization, Conv2D)): # Wenn die Schicht eine Dense-, BatchNormalization- oder Conv2D-Schicht ist
                    layer.update(learnrate, self.batch, iterations) # Ruft die Methode update der Schicht auf, um die Parameter der Schicht zu aktualisieren

            # Learning rate decay
            if learnrate_decay is not None: # Wenn eine Lernratenanpassungsfunktion angegeben ist
                learnrate = learnrate_decay(iterations, **kwargs) # Ruft die Funktion learnrate_decay auf, um die Lernrate anzupassen

            num_batches += 1 # Erh√∂ht die Anzahl der Minibatches um 1
            iterations += 1 # Erh√∂ht die Anzahl der Iterationen um 1

        cost_train /= num_batches # Berechnet den Durchschnitt der Trainingskosten f√ºr die Epoche

        # printing purpose only (Training Accuracy, Validation loss and accuracy)

        text  = f&#39;Training Loss: {round(cost_train, 4)} - &#39; # Erstellt einen Text, der die Trainingskosten enth√§lt
        self.history[&#39;Training Loss&#39;].append(cost_train) # F√ºgt die Trainingskosten zur Historie hinzu

        # training accuracy

        if self.cost_type==&#39;cross-entropy&#39;: # Wenn die Kostenfunktion die Kreuzentropie ist
            accuracy_train = np.sum(np.array(y_pred_train) == np.array(y_train)) / len(y_train) # Berechnet die Trainingsgenauigkeit f√ºr die Epoche
            text += f&#39;Training Accuracy: {round(accuracy_train, 4)}&#39; # F√ºgt die Trainingsgenauigkeit zum Text hinzu
            self.history[&#39;Training Accuracy&#39;].append(accuracy_train) # F√ºgt die Trainingsgenauigkeit zur Historie hinzu
        else: # Wenn die Kostenfunktion eine andere ist
            text += f&#39;Training Accuracy: {round(cost_train, 4)}&#39; # F√ºgt die Trainingskosten als Genauigkeit zum Text hinzu
            self.history[&#39;Training Accuracy&#39;].append(cost_train) # F√ºgt die Trainingskosten als Genauigkeit zur Historie hinzu

        if X_val is not None: # Wenn Validierungsdaten angegeben sind
            cost_val, accuracy_val = self.evaluate(X_val, y_val, batch_size) # Ruft die Methode evaluate auf, um die Validierungskosten und -genauigkeit zu berechnen
            text += f&#39; - Validation Loss: {round(cost_val, 4)} - &#39; # F√ºgt die Validierungskosten zum Text hinzu
            self.history[&#39;Validation Loss&#39;].append(cost_val) # F√ºgt die Validierungskosten zur Historie hinzu
            text += f&#39;Validation Accuracy: {round(accuracy_val, 4)}&#39; # F√ºgt die Validierungsgenauigkeit zum Text hinzu
            self.history[&#39;Validation Accuracy&#39;].append(accuracy_val) # F√ºgt die Validierungsgenauigkeit zur Historie hinzu

        if verbose:
                print(text)
        else:
            print()</code></pre>
</details>
</dd>
<dt id="Neural.Network.Network.initialize_parameters"><code class="name flex">
<span>def <span class="ident">initialize_parameters</span></span>(<span>self)</span>
</code></dt>
<dd>
<div class="desc"><p>Initialisiert die Parameter des Modells basierend auf den hinzugef√ºgten Schichten.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>self</code></strong> :&ensp;<code>object</code></dt>
<dd>Eine Instanz der Klasse Network.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>None</code></dt>
<dd>Die Methode gibt nichts zur√ºck, sondern setzt die Parameter der Schichten als Attribute der Instanz.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def initialize_parameters(self):
    &#39;&#39;&#39;
    Initialisiert die Parameter des Modells basierend auf den hinzugef√ºgten Schichten.

    Parameters
    ----------
    self : object
        Eine Instanz der Klasse Network.

    Returns
    -------
    None
        Die Methode gibt nichts zur√ºck, sondern setzt die Parameter der Schichten als Attribute der Instanz.
    &#39;&#39;&#39;
    if not self.network_architecture_called: # Wenn die Architektur des Modells noch nicht berechnet wurde
        self.network_architecture() # Ruft die Methode network_architecture auf, um die Architektur zu berechnen
        self.network_architecture_called = True # Setzt das Attribut network_architecture_called auf True
    for i, layer in enumerate(self.layers): # Iteriert √ºber jede Schicht in der Liste der Schichten
        if isinstance(layer, (Dense, Conv2D)): # Wenn die Schicht eine Dense- oder Conv2D-Schicht ist
            #print(&#34;Layer: &#34;, layer.__class__.__name__, &#34; input: &#34;, self.architecture[i])
            layer.initialize_parameters(self.architecture[i], self.optimizer_type) # Ruft die Methode initialize_parameters der Schicht auf, um die Parameter zu initialisieren
        elif isinstance(layer, BatchNormalization): # Wenn die Schicht eine BatchNormalization-Schicht ist
            layer.initialize_parameters(self.architecture[i]) # Ruft die Methode initialize_parameters der Schicht auf, um die Parameter zu initialisieren</code></pre>
</details>
</dd>
<dt id="Neural.Network.Network.loss_plot"><code class="name flex">
<span>def <span class="ident">loss_plot</span></span>(<span>self)</span>
</code></dt>
<dd>
<div class="desc"><p>Zeigt einen Plot der Trainings- und Validierungskosten pro Epoche an.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>self</code></strong> :&ensp;<code>object</code></dt>
<dd>Eine Instanz der Klasse Network.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>None</code></dt>
<dd>Die Methode gibt nichts zur√ºck, sondern zeigt den Plot auf dem Bildschirm an.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def loss_plot(self):
    &#39;&#39;&#39;
    Zeigt einen Plot der Trainings- und Validierungskosten pro Epoche an.

    Parameters
    ----------
    self : object
        Eine Instanz der Klasse Network.

    Returns
    -------
    None
        Die Methode gibt nichts zur√ºck, sondern zeigt den Plot auf dem Bildschirm an.
    &#39;&#39;&#39;
    plt.plot(self.history[&#39;Training Loss&#39;], &#39;k&#39;) # Plottet die Trainingskosten in schwarz
    if len(self.history[&#39;Validation Loss&#39;])&gt;0: # Wenn es Validierungskosten gibt
        plt.plot(self.history[&#39;Validation Loss&#39;], &#39;r&#39;) # Plottet die Validierungskosten in rot
        plt.legend([&#39;Train&#39;, &#39;Validation&#39;], loc=&#39;upper right&#39;) # F√ºgt eine Legende mit den Namen der Kurven hinzu
        plt.title(&#39;Model Loss&#39;) # F√ºgt einen Titel f√ºr den Plot hinzu
    else: # Wenn es keine Validierungskosten gibt
        plt.title(&#39;Training Loss&#39;) # F√ºgt einen Titel f√ºr den Plot hinzu
    plt.ylabel(&#39;Loss&#39;) # F√ºgt eine Beschriftung f√ºr die y-Achse hinzu
    plt.xlabel(&#39;Epoch&#39;) # F√ºgt eine Beschriftung f√ºr die x-Achse hinzu
    plt.show() # Zeigt den Plot an</code></pre>
</details>
</dd>
<dt id="Neural.Network.Network.network_architecture"><code class="name flex">
<span>def <span class="ident">network_architecture</span></span>(<span>self)</span>
</code></dt>
<dd>
<div class="desc"><p>Berechnet die Architektur des Modells basierend auf den hinzugef√ºgten Schichten.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def network_architecture(self):
    &#39;&#39;&#39;
    Berechnet die Architektur des Modells basierend auf den hinzugef√ºgten Schichten.
    &#39;&#39;&#39;
    for layer in self.layers: # Iteriert √ºber jede Schicht in der Liste
        if isinstance(layer, Conv2D): # Wenn die Schicht eine Conv2D-Schicht ist
            if layer.input_shape_x is not None: # Wenn die Schicht eine Eingabeform definiert hat
                self.Input(layer.input_shape_x) # Ruft die Input-Methode mit dieser Form auf
            layer.get_dimensions(self.architecture[-1]) # Berechnet die Ausgabeform der Schicht basierend auf der vorherigen Schicht
            self.architecture.append(layer.output_shape) # F√ºgt die Ausgabeform zur Architekturliste hinzu
            self.layer_name.append(layer.__class__.__name__) # F√ºgt den Namen der Schicht zur Namensliste hinzu
        elif isinstance(layer, (Flatten, Pooling2D)): # Wenn die Schicht eine Flatten- oder Pooling2D-Schicht ist
            layer.get_dimensions(self.architecture[-1]) # Berechnet die Ausgabeform der Schicht basierend auf der vorherigen Schicht
            self.architecture.append(layer.output_shape) # F√ºgt die Ausgabeform zur Architekturliste hinzu
            self.layer_name.append(layer.__class__.__name__) # F√ºgt den Namen der Schicht zur Namensliste hinzu
        elif isinstance(layer, Dense): # Wenn die Schicht eine Dense-Schicht ist
            self.architecture.append(layer.neurons) # F√ºgt die Anzahl der Neuronen zur Architekturliste hinzu
            self.layer_name.append(layer.__class__.__name__) # F√ºgt den Namen der Schicht zur Namensliste hinzu
        else: # Wenn die Schicht eine andere Art von Schicht ist
            self.architecture.append(self.architecture[-1]) # F√ºgt die gleiche Ausgabeform wie die vorherige Schicht zur Architekturliste hinzu
            self.layer_name.append(layer.__class__.__name__) # F√ºgt den Namen der Schicht zur Namensliste hinzu

    self.layers = list(filter(None, self.layers)) # Entfernt alle None-Elemente aus der Schichtenliste
    try:
        idx = self.layer_name.index(&#34;NoneType&#34;) # Sucht nach dem Index eines NoneType-Elements in der Namensliste
        del self.layer_name[idx] # L√∂scht das Element an diesem Index aus der Namensliste
        del self.architecture[idx] # L√∂scht das Element an diesem Index aus der Architekturliste
    except:
        pass # Wenn kein NoneType-Element gefunden wurde, tue nichts</code></pre>
</details>
</dd>
<dt id="Neural.Network.Network.predict"><code class="name flex">
<span>def <span class="ident">predict</span></span>(<span>self, X, batch_size=None)</span>
</code></dt>
<dd>
<div class="desc"><p>Erzeugt Vorhersagen des Modells f√ºr die gegebenen Eingabedaten.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>self</code></strong> :&ensp;<code>object</code></dt>
<dd>Eine Instanz der Klasse Network.</dd>
<dt><strong><code>X</code></strong> :&ensp;<code>array-like</code></dt>
<dd>Die Eingabedaten, f√ºr die das Modell Vorhersagen machen soll, z.B. ein Numpy-Array oder eine Liste von Arrays.</dd>
<dt><strong><code>batch_size</code></strong> :&ensp;<code>int</code>, optional</dt>
<dd>Die Gr√∂√üe der Minibatches, die f√ºr die Vorhersage verwendet werden sollen. Wenn None, wird die L√§nge von X verwendet. Der Standardwert ist None.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><strong><code>y_pred</code></strong> :&ensp;<code>array-like</code></dt>
<dd>Die Vorhersagen des Modells f√ºr die Eingabedaten, z.B. ein Numpy-Array oder eine Liste von Arrays.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def predict(self, X, batch_size=None):
    &#39;&#39;&#39;
    Erzeugt Vorhersagen des Modells f√ºr die gegebenen Eingabedaten.

    Parameters
    ----------
    self : object
        Eine Instanz der Klasse Network.
    X : array-like
        Die Eingabedaten, f√ºr die das Modell Vorhersagen machen soll, z.B. ein Numpy-Array oder eine Liste von Arrays.
    batch_size : int, optional
        Die Gr√∂√üe der Minibatches, die f√ºr die Vorhersage verwendet werden sollen. Wenn None, wird die L√§nge von X verwendet. Der Standardwert ist None.

    Returns
    -------
    y_pred : array-like
        Die Vorhersagen des Modells f√ºr die Eingabedaten, z.B. ein Numpy-Array oder eine Liste von Arrays.
    &#39;&#39;&#39;
    if batch_size==None: # Wenn keine Batch-Gr√∂√üe angegeben ist
        batch_size = len(X) # Verwendet die L√§nge von X als Batch-Gr√∂√üe

    for i in range(0, len(X), batch_size): # Iteriert √ºber jede Minibatch
        X_batch = X[i:i+batch_size] # Extrahiert die Eingabedaten f√ºr die Minibatch
        Z = X_batch.copy() # Erstellt eine Kopie der Eingabedaten
        for layer in self.layers: # Iteriert √ºber jede Schicht im Modell
            if layer.__class__.__name__==&#39;BatchNormalization&#39;: # Wenn die Schicht eine BatchNormalization-Schicht ist
                Z = layer.forward(Z, mode=&#39;test&#39;) # Ruft die Methode forward der Schicht auf, um die Ausgabe der Schicht im Testmodus zu berechnen
            else: # Wenn die Schicht eine andere Art von Schicht ist
                Z = layer.forward(Z) # Ruft die Methode forward der Schicht auf, um die Ausgabe der Schicht zu berechnen
        if i==0: # Wenn dies die erste Minibatch ist
            if self.cost_type==&#39;cross-entropy&#39;: # Wenn die Kostenfunktion die Kreuzentropie ist
                y_pred = np.argmax(Z, axis=1).tolist() # Berechnet die Vorhersagen des Modells f√ºr die Minibatch als eine Liste von Indizes
            else: # Wenn die Kostenfunktion eine andere ist
                y_pred = Z # Speichert die Ausgabe des Modells f√ºr die Minibatch als ein Array
        else: # Wenn dies nicht die erste Minibatch ist
            if self.cost_type==&#39;cross-entropy&#39;: # Wenn die Kostenfunktion die Kreuzentropie ist
                y_pred += np.argmax(Z, axis=1).tolist() # F√ºgt die Vorhersagen des Modells f√ºr die Minibatch zur Liste der Vorhersagen hinzu
            else: # Wenn die Kostenfunktion eine andere ist
                y_pred = np.vstack((y_pred, Z)) # Stapelt die Ausgabe des Modells f√ºr die Minibatch unter der bisherigen Ausgabe

    return np.array(y_pred) # Gibt die Vorhersagen des Modells f√ºr die Eingabedaten als ein Array zur√ºck</code></pre>
</details>
</dd>
<dt id="Neural.Network.Network.summary"><code class="name flex">
<span>def <span class="ident">summary</span></span>(<span>self)</span>
</code></dt>
<dd>
<div class="desc"><p>Zeigt eine Zusammenfassung des Modells an, einschlie√ülich der Schichttypen, der Ausgabeformen und der Anzahl der Parameter.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>self</code></strong> :&ensp;<code>object</code></dt>
<dd>Eine Instanz der Klasse Network.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>None</code></dt>
<dd>Die Methode gibt nichts zur√ºck, sondern druckt die Zusammenfassung auf dem Bildschirm aus.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def summary(self):
    &#39;&#39;&#39;
    Zeigt eine Zusammenfassung des Modells an, einschlie√ülich der Schichttypen, der Ausgabeformen und der Anzahl der Parameter.

    Parameters
    ----------
    self : object
        Eine Instanz der Klasse Network.

    Returns
    -------
    None
        Die Methode gibt nichts zur√ºck, sondern druckt die Zusammenfassung auf dem Bildschirm aus.
    &#39;&#39;&#39;
    if self.network_architecture_called==False: # Wenn die Architektur des Modells noch nicht berechnet wurde
        self.network_architecture() # Ruft die Methode network_architecture auf, um die Architektur zu berechnen
        self.network_architecture_called = True # Setzt das Attribut network_architecture_called auf True
    len_assigned = [45, 26, 15] # Eine Liste von L√§ngen, die f√ºr die Spalten der Zusammenfassung zugewiesen werden
    count = {&#39;Dense&#39;: 1, &#39;Activation&#39;: 1, &#39;Input&#39;: 1,
            &#39;BatchNormalization&#39;: 1, &#39;Dropout&#39;: 1, &#39;Conv2D&#39;: 1,
            &#39;Pooling2D&#39;: 1, &#39;Flatten&#39;: 1} # Ein W√∂rterbuch, das die Anzahl jeder Schichtart speichert

    col_names = [&#39;Layer (type)&#39;, &#39;Output Shape&#39;, &#39;# of Parameters&#39;] # Eine Liste von Spaltennamen f√ºr die Zusammenfassung

    print(&#34;Model: CNN&#34;) # Druckt den Namen des Modells
    print(&#39;-&#39;*sum(len_assigned)) # Druckt eine Trennlinie
    
    text = &#39;&#39; # Initialisiert einen leeren Text
    for i in range(3): # Iteriert √ºber die drei Spalten
        text += col_names[i] + &#39; &#39;*(len_assigned[i]-len(col_names[i])) # F√ºgt den Spaltennamen und die erforderlichen Leerzeichen zum Text hinzu
    print(text) # Druckt den Text

    print(&#39;=&#39;*sum(len_assigned)) # Druckt eine Trennlinie

    total_params = 0 # Initialisiert die Gesamtzahl der Parameter auf 0
    trainable_params = 0 # Initialisiert die Anzahl der trainierbaren Parameter auf 0
    non_trainable_params = 0 # Initialisiert die Anzahl der nicht trainierbaren Parameter auf 0

    for i in range(len(self.layer_name)): # Iteriert √ºber jede Schicht in der Namensliste
        # layer name
        layer_name = self.layer_name[i] # Speichert den Namen der Schicht
        name = layer_name.lower() + &#39;_&#39; + str(count[layer_name]) + &#39; &#39; + &#39;(&#39; + layer_name + &#39;)&#39; # Erstellt einen eindeutigen Namen f√ºr die Schicht mit ihrer Nummer und ihrem Typ
        count[layer_name] += 1 # Erh√∂ht die Anzahl dieser Schichtart um 1

        # output shape
        try: # Versucht, die Ausgabeform der Schicht als Tupel zu erstellen
            out = &#39;(None, &#39; # Beginnt das Tupel mit None f√ºr die Batch-Dimension
            for n in range(len(self.architecture[i])-1): # Iteriert √ºber die restlichen Dimensionen au√üer der letzten
                out += str(self.architecture[i][n]) + &#39;, &#39; # F√ºgt die Dimension und ein Komma zum Tupel hinzu
            out += str(self.architecture[i][-1]) + &#39;)&#39; # F√ºgt die letzte Dimension und eine schlie√üende Klammer zum Tupel hinzu
        except: # Wenn die Ausgabeform keine Tupel ist
            out = &#39;(None, &#39; + str(self.architecture[i]) + &#39;)&#39; # Erstellt die Ausgabeform als Tupel mit nur einer Dimension

        # number of params
        if layer_name==&#39;Dense&#39;: # Wenn die Schicht eine Dense-Schicht ist
            h0 = self.architecture[i-1] # Speichert die Anzahl der Eingangsneuronen
            h1 = self.architecture[i] # Speichert die Anzahl der Ausgangsneuronen
            if self.layers[i-1].use_bias: # Wenn die Schicht einen Bias-Vektor verwendet
                params = h0*h1 + h1 # Berechnet die Anzahl der Parameter als das Produkt der Neuronen plus die Anzahl der Ausgangsneuronen
            else: # Wenn die Schicht keinen Bias-Vektor verwendet
                params = h0*h1 # Berechnet die Anzahl der Parameter als das Produkt der Neuronen
            total_params += params # Addiert die Anzahl der Parameter zur Gesamtzahl hinzu
            trainable_params += params # Addiert die Anzahl der Parameter zur Anzahl der trainierbaren Parameter hinzu
        elif layer_name==&#39;BatchNormalization&#39;: # Wenn die Schicht eine BatchNormalization-Schicht ist
            h = self.architecture[i] # Speichert die Anzahl der Merkmale
            params = 4*h # Berechnet die Anzahl der Parameter als das Vierfache der Merkmale
            trainable_params += 2*h # Addiert die H√§lfte der Parameter zur Anzahl der trainierbaren Parameter hinzu
            non_trainable_params += 2*h # Addiert die H√§lfte der Parameter zur Anzahl der nicht trainierbaren Parameter hinzu
            total_params += params # Addiert die Anzahl der Parameter zur Gesamtzahl hinzu
        elif layer_name==&#39;Conv2D&#39;: # Wenn die Schicht eine Conv2D-Schicht ist
            layer = self.layers[i-1] # Speichert die Schicht als ein Objekt
            if layer.use_bias: # Wenn die Schicht einen Bias-Vektor verwendet
                add_b = 1 # Speichert eine zus√§tzliche Einheit f√ºr den Bias
            else: # Wenn die Schicht keinen Bias-Vektor verwendet
                add_b = 0 # Speichert keine zus√§tzliche Einheit f√ºr den Bias
            params = ((layer.inputC * layer.kernelH * layer.kernelW) + add_b) * layer.F # Berechnet die Anzahl der Parameter als das Produkt der Eingangskan√§le, der Kernelh√∂he, der Kernelbreite und der Anzahl der Filter plus die zus√§tzliche Einheit f√ºr den Bias
            trainable_params += params # Addiert die Anzahl der Parameter zur Anzahl der trainierbaren Parameter hinzu
            total_params += params # Addiert die Anzahl der Parameter zur Gesamtzahl hinzu
        else: # Wenn die Schicht eine andere Art von Schicht ist
            # Pooling, Dropout, Flatten, Input
            params = 0 # Speichert die Anzahl der Parameter als 0
        names = [name, out, str(params)] # Erstellt eine Liste mit dem Namen, der Ausgabeform und der Anzahl der Parameter der Schicht

        # print this row
        text = &#39;&#39; # Initialisiert einen leeren Text
        for j in range(3): # Iteriert √ºber die drei Spalten
            text += names[j] + &#39; &#39;*(len_assigned[j]-len(names[j])) # F√ºgt den Namen, die Ausgabeform oder die Anzahl der Parameter und die erforderlichen Leerzeichen zum Text hinzu
        print(text) # Druckt den Text
        if i!=(len(self.layer_name)-1): # Wenn dies nicht die letzte Schicht ist
            print(&#39;-&#39;*sum(len_assigned)) # Druckt eine Trennlinie
        else: # Wenn dies die letzte Schicht ist
            print(&#39;=&#39;*sum(len_assigned)) # Druckt eine Trennlinie

    print(&#34;Total params:&#34;, total_params) # Druckt die Gesamtzahl der Parameter
    print(&#34;Trainable params:&#34;, trainable_params) # Druckt die Anzahl der trainierbaren Parameter
    print(&#34;Non-trainable params:&#34;, non_trainable_params) # Druckt die Anzahl der nicht trainierbaren Parameter
    print(&#39;-&#39;*sum(len_assigned)) # Druckt eine Trennlinie</code></pre>
</details>
</dd>
</dl>
</dd>
</dl>
</section>
</article>
<nav id="sidebar">
<h1>Index</h1>
<div class="toc">
<ul></ul>
</div>
<ul id="index">
<li><h3>Super-module</h3>
<ul>
<li><code><a title="Neural" href="index.html">Neural</a></code></li>
</ul>
</li>
<li><h3><a href="#header-classes">Classes</a></h3>
<ul>
<li>
<h4><code><a title="Neural.Network.BatchNormalization" href="#Neural.Network.BatchNormalization">BatchNormalization</a></code></h4>
<ul class="">
<li><code><a title="Neural.Network.BatchNormalization.backpropagation" href="#Neural.Network.BatchNormalization.backpropagation">backpropagation</a></code></li>
<li><code><a title="Neural.Network.BatchNormalization.forward" href="#Neural.Network.BatchNormalization.forward">forward</a></code></li>
<li><code><a title="Neural.Network.BatchNormalization.initialize_parameters" href="#Neural.Network.BatchNormalization.initialize_parameters">initialize_parameters</a></code></li>
<li><code><a title="Neural.Network.BatchNormalization.update" href="#Neural.Network.BatchNormalization.update">update</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="Neural.Network.Network" href="#Neural.Network.Network">Network</a></code></h4>
<ul class="">
<li><code><a title="Neural.Network.Network.Input" href="#Neural.Network.Network.Input">Input</a></code></li>
<li><code><a title="Neural.Network.Network.accuracy_plot" href="#Neural.Network.Network.accuracy_plot">accuracy_plot</a></code></li>
<li><code><a title="Neural.Network.Network.add" href="#Neural.Network.Network.add">add</a></code></li>
<li><code><a title="Neural.Network.Network.compile" href="#Neural.Network.Network.compile">compile</a></code></li>
<li><code><a title="Neural.Network.Network.evaluate" href="#Neural.Network.Network.evaluate">evaluate</a></code></li>
<li><code><a title="Neural.Network.Network.fit" href="#Neural.Network.Network.fit">fit</a></code></li>
<li><code><a title="Neural.Network.Network.initialize_parameters" href="#Neural.Network.Network.initialize_parameters">initialize_parameters</a></code></li>
<li><code><a title="Neural.Network.Network.loss_plot" href="#Neural.Network.Network.loss_plot">loss_plot</a></code></li>
<li><code><a title="Neural.Network.Network.network_architecture" href="#Neural.Network.Network.network_architecture">network_architecture</a></code></li>
<li><code><a title="Neural.Network.Network.predict" href="#Neural.Network.Network.predict">predict</a></code></li>
<li><code><a title="Neural.Network.Network.summary" href="#Neural.Network.Network.summary">summary</a></code></li>
</ul>
</li>
</ul>
</li>
</ul>
</nav>
</main>
<footer id="footer">
<p>Generated by <a href="https://pdoc3.github.io/pdoc" title="pdoc: Python API documentation generator"><cite>pdoc</cite> 0.10.0</a>.</p>
</footer>
</body>
</html>